{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider the problem of mining frequent itemsets when the input is a collection of transactions, where a transaction is again an itemset. The frequent itemsets we want to mine from the transactions will be subsets of these transactions, and so we are interested in discovering the most frequent subsets in such transactions. We are going to present two widely used algorithms: the *A-Priori algorithm* and the *FP-Growth algorithm*. The last one is implemented in the machine learning library of spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary start-up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python2 not 3 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "#os.environ['PYTHONPATH'] = ':'.join(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print ( spark_home )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print ( sc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The context where this problem has been defined and studied extensively is the analysis of costumer habits in traditional retail stores. We consider mining frequent itemsets from a sequence of sets, these sets are traditionally called transactions or baskets, because the original application domain where this problem has been studied has been discovering frequently bought together products (items) in different purchases (baskets or transactions). In this context, given a frequency threshold $\\theta$, and a sequence of sets $S_1,S_2,...,S_M$, we say that a itemset $P \\subseteq I$ is frequent if its support among  $S_1,S_2,...,S_M$ is $ \\geq \\theta  $, where the support is the fraction of sets that contain $P$. As we will see when we talk of recommender systems, in the case of recommending products for on-line shops where many clients buy many different things, the kinds of problems considered are different to this frequent itemset problem.\n",
    "\n",
    "For a tradicional retail store, finding frequent itemsets can be important towards directing their marketing efforts towards sets of products that **many** costumers will find interesting, but in the setting of on-line shops, the recommendations can be (or **should be**) personalized for each costumer, so this makes the domains (tradicional and on-line) different regarding the kind of problems they want to solve for finding marketing strategies.  However, even for on-line stores we can find sometimes some small subsets of products that are bought together very frequently (e.g. people that buy iPhones may tend to buy particular protective cases).\n",
    "\n",
    "\n",
    "First, we are going to present the most well-known algorithm for solving this problem, the A-Priori algorithm. It is not the most efficient nowadays, for many cases, but it is a simple algorithm that shows clearly the difficulties in solving this problem. We will also discuss the idea behind a more advanced algorithm, the FP-Growth algorithm, that has also an specific efficient parallel version in the map-reduce framework that it is implemented in the machine library of spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The A-Priori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This algorithm is based on exploiting the following basic principle, the monotonicity of itemsets:\n",
    "\n",
    "         If a set P of items is frequent, then so is every subset of P\n",
    "            \n",
    "Observe that if this is true for $P$ with frequency $\\theta$, it will be true for every subset of $P$ with frequency *at least* $\\theta$. The typical situation will be that for a fixed frequency $ \\theta $, we will find much less frequent sets of size $k$ than of size $k-1$. In the particular domain of tradicional (brick-and-mortar) retail stores, the typical transaction size will be small, so when considering high frequency thresholds, we will not find many frequent item sets of big size.  \n",
    "\n",
    "So, the A-Priori algorithm is based on first finding smaller frequent-items sets, and then going to the next size but considering only sets such that any of their subsets has been found previously to be frequent. This suggests an iterative algorithm that first looks for frequent itemsets of size 1, then of size 2, and so on.\n",
    "\n",
    "Here you have a very high-level pseudo-code of the A-Priori algorithm:\n",
    "\n",
    ">Find frequent singleton-sets (k=1), store them in $L_1$  \n",
    ">k=2  \n",
    ">While ($L_{k-1}$ is not empty) do:  \n",
    "    >>Define $C_k$ to be the set of itemsets of size k, where every subset of size $k-1$ is in $L_{k-1}$  \n",
    "    >>For every transaction B, find all its itemsets of size k that are in $C_k$  \n",
    "    >>Add to $L_k$ any the previous itemsets of size $k$ that has been found in at least a $\\theta$ fraction of the transactions    \n",
    "    >>k=k+1\n",
    ">\n",
    "\n",
    "Where is the bottleneck performance in a possible implementation of this algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical (usable) implementation of the general A-Priori algorithm should use specialized data structures to low the complexity of the candidate set generation and filtering steps. However, we here consider an implementation for the particular case of only computing $L_1$ and $L_2$ sets. I include here the full code for computing L1, but for L2 I only include a partial code, that **YOU should finish it as a programming exercise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From a transaction in a single string with items separated by white spaces\n",
    "# generate a list with the items of the transaction\n",
    "def parseTransaction( trans ):\n",
    "    return trans.strip().split(' ')\n",
    "\n",
    "# Compute the rdd with frequent singletonsets (L_1)\n",
    "def computeL1 ( rddtrans, numtrans, theta ):\n",
    "  rddtemp = rddtrans.flatMap( lambda trans : [ (it,1) for it in trans ] ).reduceByKey( lambda a,b : a+b  )\n",
    "  rddL1 = rddtemp.filter( lambda x : (float(x[1])/numtrans) >= theta )\n",
    "  return rddL1\n",
    "\n",
    "# This is a function to map transactions to transactions with only items in L1,\n",
    "# something that can increase the efficiency when computing L2\n",
    "def filterOutL1( transseq, L1 ):\n",
    "    for trans in transseq:\n",
    "       yield [ it for it in trans if (it in L1) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Finish the implementation of the A-priori algorithm for k up to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#      HERE ARE THE TWO FUNCTIONS YOU SHOULD FINISH\n",
    "# \n",
    "\n",
    "# Compute L2 from transactions and L1. This function assumes\n",
    "# that the input rddtrans has been simplified by eliminating from the transactions those items not in L1\n",
    "# Also, a generalized version of this function for any size k should use\n",
    "# more specialized data structures to test-and-check candidate itemsets for L_k from L_{k-1}\n",
    "\n",
    "# First, the function to generate candidate frequent pairs (C_2)\n",
    "# for each transaction of a partition of the set of transactions\n",
    "# This function expects a sequence of transactions, so it should be used with \n",
    "# mapPartitions\n",
    "#\n",
    "def generateC2( seqoftransactions,  L1  ):\n",
    "    for trans in seqoftransactions:\n",
    "      itemsetlist = []\n",
    "      #\n",
    "      #   INSERT CODE HERE for computing all the pairs (a,b) of items\n",
    "      #    in trans such that both iterms belong to L1 and store them\n",
    "      #   in itemsetlist as (key,value) pairs of the form  ((a,b),1)\n",
    "      yield itemsetlist\n",
    "    \n",
    "# And finally, the function for computing L_2 from L_1 and the rdd with\n",
    "# the set of transactions\n",
    "# This function should use your previous generateC2 function to compute L2 from C2\n",
    "#\n",
    "def computeL2( rddtrans, numtrans, L1, theta ):\n",
    "    #   Map the transactions in rddtrans to the set of candidate frquenquent pairs (C2) \n",
    "    #   using the previous function generateC2\n",
    "    candidatePairsrdd = \n",
    "    #  Count number of occurences in the transactions for  each pair in C2. \n",
    "    #  Use flatpMap and reduceByKey to get the final rdd\n",
    "    rddL2temp = \n",
    "    # Finally, filter out from the previous rdd those pairs with frequency below theta\n",
    "    rddL2 = \n",
    "    return rddL2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with the sample set in the file \"sample_fpgrowth.txt\" :\n",
    "\n",
    "> r z h k p  \n",
    "> z y x w v u t s   \n",
    "> s x o n r  \n",
    "> x z y m t s q e  \n",
    "> z  \n",
    "> x z y r q t p  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Once you finish your code, test it with this cell and the next one.\n",
    "# Load set of transactions as a sequence of itemsets (every transaction stored in a list)\n",
    "#\n",
    "rddtrans = sc.textFile( spark_home+\"/data/mllib/sample_fpgrowth.txt\" ).map(parseTransaction)\n",
    "numtrans = rddtrans.count()\n",
    "\n",
    "\n",
    "# First, let's compute the frequent singleton sets with theta=0.25\n",
    "#\n",
    "rddL1 = computeL1( rddtrans, float(numtrans), 0.25  )\n",
    "L1 = rddL1.collect()\n",
    "print (\"L1 together with frequency information: \\n\", L1 )\n",
    "\n",
    "# Compute a version without the frequency information:\n",
    "#\n",
    "L1onlyitems = rddL1.map( lambda it : it[0] ).collect()\n",
    "print ( \"L1 only items: \\n\", L1onlyitems )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you finish the code for functions generateC2 and computeL2, you can try execute the next code cell to compute the set of frequent pairs (L2). For the sample file , this is the set L2 you should get (for frequency 0.25):\n",
    "\n",
    "> pairs with frequency information :  [((u't', u'x'), 3), ((u'p', u'r'), 2), ((u't', u'z'), 3), ((u's', u'y'), 2), ((u'r', u'z'), 2), ((u'x', u'z'), 3), ((u'q', u'y'), 2), ((u'r', u'x'), 2), ((u'p', u'z'), 2), ((u's', u'x'), 3), ((u'y', u'z'), 3), ((u's', u'z'), 2), ((u'q', u'z'), 2), ((u'q', u'x'), 2), ((u'q', u't'), 2), ((u's', u't'), 2), ((u'x', u'y'), 3), ((u't', u'y'), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, compute frequent pairs (L2) from frequent items (L1)\n",
    "# We  need to work with the collected back version (to the driver) of the L1 set\n",
    "# (This will be Ok as far as L1 is small enough to fit in the memory\n",
    "# of every single machine ). We use mapPartitions to distribute only one function call per partition,\n",
    "# avoiding the overhead of executing many functions with the parameter L1\n",
    "#\n",
    "L1filteredtransrdd = rddtrans.mapPartitions( lambda transseq : filterOutL1( transseq, L1onlyitems )  )\n",
    "\n",
    "# Let's check if the filtered transactions are correct:\n",
    "#\n",
    "print (\" transactions with only L1 items : \", L1filteredtransrdd.collect() )\n",
    "\n",
    "# Next, compute the frequent pairs (L_2)\n",
    "#\n",
    "rddL2 = computeL2( L1filteredtransrdd, float(numtrans), L1onlyitems, 0.25 )\n",
    "print ( \"\\n pairs with frequency information : \", rddL2.collect() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The original application domain of the frequent itemsets problem was finding good marketing strategies in traditional retail stores. For example, in a famous application of this problem, it was found that a frequent itemset in costumer transactions was the pair {diapers,beer}, but that the frequency of the singleton set {beer} in the whole set of transactions was much smaller than the frequency of  {diaper,beer} in the transactions with {diapers}. We can quantify this situation with two measures associated with the rule. We say that the rule {diapers} -> {beer} has a high **confidence** if the ratio:\n",
    "\n",
    "> support( {diapers,beer} ) / support( {diapers})                        \n",
    "                           \n",
    "is high (the fraction of itemsets with diapers that also contain beer is high). But observe that this alone does not mean that the rule {diapers} -> {beer} shows a *true relationship*. That is, it could happen that all the costumers buy beer, so the  rule {diapers} -> {beer} does not really provide an useful information to discover when people buys **more beer**. To quantify this situation, we need also to quantify the **interest** of the rule {I} -> {j} as the difference between its confidence and the frequency of {j}:\n",
    "\n",
    "> confidence ( {diapers} -> {beer} ) - frequency( {beer} ) \n",
    "\n",
    "Observe that the interest can be positive, zero o negative. For the diapers-beer example mentioned, the interest was positive, meaning that when diapers was present the relative frequency of beer was higher than when looking for beer in all the transactions. A negative interest would indicate a negative effect, i.e. that when {I} is present {j} is less likely to be present than in general.\n",
    "\n",
    "So, towards building such marketing recommendation system, it can be interesting to advance one more step and build association rules that are interesting for this final marketing task. For example, given a set of frequent itemsets, we can compute the confidence and interest of any association rule of the kind {I} -> {j} we can build with the frequent itemsets. From a given frequent itemset P, observe that we can build |P|-1 different association rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To consider the difference between the confidence and the interest of an association rule consider the two following transaction sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beeranddiapers1 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['beer','diapers','milk','pizza'],\n",
    "                   ['beer','diapers','toothpaste'],\n",
    "                   ['beer','diapers','icecream'],\n",
    "                   ['beer','diapers','pizza','yogurt']]\n",
    "\n",
    "beeranddiapers2 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['yogurt','milk','pizza'],\n",
    "                   ['pizza','toothpaste'],\n",
    "                   ['yogurt','icecream'],\n",
    "                   ['beer','pizza','yogurt']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that the confidence for the rule: \n",
    "> {diapers} -> {beer}\n",
    "\n",
    "  is:\n",
    "\n",
    "> $ \\frac{support(\\{beer,diapers\\})}{support(\\{diapers\\})} = \\frac{8}{8} = 1 $  (for the first set)  \n",
    "> $ \\frac{support(\\{beer,diapers\\})}{support(\\{diapers\\})} = \\frac{4}{4} = 1 $  (for the second set)  \n",
    "\n",
    "So, for both sets the confidence of the rule is maximum. However, we observe that the rule is really more useful for the second transaction set because the interest for the first transaction set is:\n",
    "\n",
    "> $ confidence ( \\{diapers\\} \\rightarrow \\{beer\\} ) - frequency( \\{beer\\} ) = 1 - 1 = 0$\n",
    "\n",
    "but for the second set is: \n",
    "\n",
    "> $ confidence ( \\{diapers\\} \\rightarrow \\{beer\\} ) - frequency( \\{beer\\} ) = 1 - (5/8) = 0.375 $\n",
    "\n",
    "That is, in the second transaction set the presence of diapers makes more likely the presence of beer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Consider the following programming exercise. Given the information of the frequent singletons and frequent pairs sets we generated with our previous implementation, implement the following functions to generate the association rules linked with the frequent itemsets together with its confidence and its interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Given a:\n",
    "#    - a frequent pair with its support information  like (('t', 'x'), 3)\n",
    "#    - an item j from the pair like  't'\n",
    "#    - the number of transactions\n",
    "#    - a list with the L1 information computed with the function computeL1 and then collected back\n",
    "#    to the driver\n",
    "#\n",
    "#   Compute the confidence and interest of the rule  freqitemset-{j} ->  {j}\n",
    "#   Return it with the format:  ( 'freqitemset-{j} ->  j', confidence, interest )\n",
    "def compConfidenceAndInterestForRule( freqpairWithFreq, j, numtrans, L1withfreqinfo ):\n",
    "    #\n",
    "    # INSERT YOUR SOLUTION HERE\n",
    "    #\n",
    "    premise = \n",
    "    confidence =     \n",
    "    freqj = \n",
    "    interest = confidence - freqj\n",
    "    return ( str(premise)+\" -> \"+str(j), confidence, interest  )\n",
    "    \n",
    "\n",
    "#\n",
    "#    Map each freqpair of a partition, to its SET of different association rules\n",
    "#    together with their confidence and interest\n",
    "#\n",
    "#    This function should use the previous compConfidenceAndInterestForRule for each\n",
    "#    association rule generated\n",
    "#\n",
    "def genAssocRulesForPartition( freqitemsetsequence, numtrans, L1withfreqinfo ):\n",
    "    for fpair in freqitemsetsequence:\n",
    "        rulesforpair = []\n",
    "        # INSERT YOUR SOLUTION HERE    \n",
    "        yield rulesforpair \n",
    "\n",
    "#\n",
    "#   Finally, map all the frequenitemsets of all the partitions of the input\n",
    "#   rddfreqsets using the previous function, and finally collect back all\n",
    "#   the association rules obtained, but show them on the screen ordered by\n",
    "#   descending order, using their interest to orden them\n",
    "#\n",
    "def mapFreqItemSetsToAssocRules( rddfreqsets, numtrans, L1withfreqinfo ):\n",
    "    # Compute an rdd with the assotiation rules for the freqsets, calling the previous function\n",
    "    # for each partition of the rddfreqsets \n",
    "    #   INSERT CODE HERE \n",
    "    #\n",
    "    # Next, get a \"flat\" version of the previous set of rules (one rule per element of the RDD)\n",
    "    #   INSERT CODE HERE\n",
    "    #\n",
    "    # Next, collect back the obtained association rules, but sort them in descending order by their interest\n",
    "    # You can either sort in a new rdd and then collect back the sorted rdd, or sort them in the driver after\n",
    "    # collecting back the set of association rules.\n",
    "    #\n",
    "    #   INSERT YOUR SOLUTION HERE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Next, test your code with the RDD of frequent pairs rddL2 computed before\n",
    "# and its corresponding set of frequent singletons L1\n",
    "#\n",
    "#   INSERT YOUR SOLUTION HERE:\n",
    "\n",
    "\n",
    "# You can try your code also with the transaction sets beeranddiapers1 and beeranddiapers2\n",
    "# (Compute first L1 and L2 for them for computing their association rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discussed in the A-Priori frequent itemsets mining algorithm that a major bottleneck in the performance is the possible increasing number of candidate itemsets as the size $k$ considered increases in every stage of the algorithm. We are next going to present the basic idea behind an algorithm, the FP-Growth, that avoids this candidate set generation of itemsets of size k ($C_k$) to be able to generate the actual set of frequent itemsets of size k ($L_k$). The algorithm achieves this improvement thanks to a data structure called a **prefix-search tree**. This tree is a *compact* representation of the set of transactions, but in such a way that allows a traversal of its branches to efficiently find the frequent itemsets of any size k scanning the tree only once.\n",
    "\n",
    "Consider the following set of transactions:\n",
    "\n",
    "> f c a m p i   \n",
    "> f c a b m o   \n",
    "> f b  j  \n",
    "> c b p s   \n",
    "> f c a m p l    \n",
    "\n",
    "\n",
    "Then, its corresponding prefix-search tree, when considering minSuport=3/5 and its corresponding ordered set of singleton sets L1 with support at least 3/5: $ \\{ f:4, c:4, a:3, b:3, m:3, p:3 \\} $,  is the following one:\n",
    "\n",
    "<img src=\"exampleFPtree.png\" width=\"250px\" />\n",
    "\n",
    "This prefix-search tree represents in a compact way the four transactions that contain the most frequent element (f), with the subtree with root f. Observe that this subtree contains three subbranches (that share some nodes), where the branch f-c-a-m-p encodes the information of the first and last transactions of the DB, the branch f-c-a-b-m encodes the second transaction, and the branch f-b the third transaction. Finally, the branch c-b-p represents the fourth transaction. The number that labels each node represents the number of transactions of the DB that contain the subset given by the path of items from the root to the node.\n",
    "\n",
    "Given a DB of transactions and the ordered set L1 computed with a particular minSupport, the following algorithm (pseudo-code) can be used to build such prefix-search tree\n",
    "\n",
    "```python\n",
    " def buildFPtree( L1, DB ):\n",
    "   # create initial tree with only a root\n",
    "   fptree = (root,())\n",
    "   L1 = sortByFrequency(L1)\n",
    "   for tran in DB:\n",
    "      sortedtran = sortByFrequency(tran,L1)\n",
    "      insertTree(sortedtran,fptree)\n",
    "   return fptree\n",
    "   \n",
    " def insertTree( sortedtran, fptree ):\n",
    "   if (sortedtran is empty):\n",
    "      return\n",
    "   firstit = head(sortedtran)\n",
    "   if (fptree has child node with name firstit):\n",
    "       # get subtree with root named firstit\n",
    "       fpsubtree = getsubtree( fptree, firstit )\n",
    "       incrementRootCounter( fpsubtree )\n",
    "   else:\n",
    "      # create new subtree an attach it as child of fptree\n",
    "      fpsubtree = ((firstit,1),())\n",
    "      addchild(fptree,fpsubtree)\n",
    "   # process remaining items of the transaction over the subtree\n",
    "   tailtran = sortedtran.delete(firstit)\n",
    "   insertTree( tailtran, fpsubtree )   \n",
    "   \n",
    "```\n",
    "\n",
    "\n",
    "Observe that the overall process for building the prefix-search tree needs two scans of the transaction DB, one for building the ordered set L1, and a second one for building the tree from L1 and the DB of transactions, where each transaction is processed once.\n",
    "\n",
    "\n",
    "Once we have such tree, it is possible to compute the **full set of frequent subsets** with the given minSupport, from our DB of transactions with a recursive algorithm that traverses the tree recursively. The details of such algorithm (and of the previous prefix-search tree construction) can be found in the following paper:\n",
    "\n",
    "\n",
    ">  Jiawei Han, Jian Pei, Yiwen Yin, Runying Mao. *Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach*. Data Mining Knowledge Discovery. 8(1): 53-87 (2004).\n",
    "> Link: http://hanj.cs.illinois.edu/pdf/dami04_fptree.pdf\n",
    "\n",
    "\n",
    "Spark includes a distributed version of FPgrowth, that works by creating different independent prefix-search trees in different partitions and finally merging the resulting frequent subsets of each partition, described in the paper:\n",
    "\n",
    ">  Haoyuan Li, Yi Wang, Dong Zhang, Ming Zhang, Edward Y. Chang. *PFP: parallel FP-growth for query recommendation*. RecSys 2008: 107-114.\n",
    "> Link: http://dl.acm.org/citation.cfm?doid=1454008.1454027\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's see this algorithm in action when looking for all the frequent subsets of the sample file sample_fpgrowth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['r'], freq=3)\n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['z'], freq=5)\n",
      "FreqItemset(items=['s'], freq=3)\n",
      "FreqItemset(items=['s', 't'], freq=2)\n",
      "FreqItemset(items=['s', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 't'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['x'], freq=4)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['t'], freq=3)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p'], freq=2)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 't'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n",
      "FreqItemset(items=['y'], freq=3)\n",
      "FreqItemset(items=['y', 't'], freq=3)\n",
      "FreqItemset(items=['y', 't', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 't', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 't', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile( spark_home+\"/data/mllib/sample_fpgrowth.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.25, numPartitions=4)\n",
    "\n",
    "# Collect back the set of ALL frequent itemsets with minSupport\n",
    "# Beware !, this set could be exponientally large with respect to the set of\n",
    "# input transactions !\n",
    "#\n",
    "rddfreqitemsets = model.freqItemsets()\n",
    "result = rddfreqitemsets.collect()\n",
    "for fi in result:\n",
    "    print( fi )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However, because model.freqItemsets() is a RDD, we can instead save them to a data file, or filter only some of them to be analyzed by our program back in the driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent pairs: \n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 't'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n",
      "FreqItemset(items=['y', 't'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n",
      "\n",
      "Frequent triples: \n",
      "FreqItemset(items=['s', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 't'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 't'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['y', 't', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 't', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n"
     ]
    }
   ],
   "source": [
    "pairs = rddfreqitemsets.filter( lambda x  : len(x.items) == 2  ).collect()\n",
    "\n",
    "print ( \"Frequent pairs: \" )\n",
    "for fi in pairs:\n",
    "    print( fi )\n",
    "    \n",
    "print ( \"\\nFrequent triples: \" )\n",
    "triples = rddfreqitemsets.filter( lambda x  : len(x.items) == 3  ).collect()    \n",
    "for fi in triples:\n",
    "    print ( fi )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can also use the function RDD.toLocalIterator() that given a RDD allows you to iterate though all the elements of the RDD in the driver program. So in case you want to perform some individual final task on each resulting frequent subset you could use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Frequent subset:  FreqItemset(items=['r'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['r', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['r', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['z'], freq=5)\n",
      " Frequent subset:  FreqItemset(items=['s'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['s', 't'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 't', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 't', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 't', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'x'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 't'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 't', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 't', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 't', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['s', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['x'], freq=4)\n",
      " Frequent subset:  FreqItemset(items=['x', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['t'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['t', 'x'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['t', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['p'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['p', 'r'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['p', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 't'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 't'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 't', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 't', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 't', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['q', 'z'], freq=2)\n",
      " Frequent subset:  FreqItemset(items=['y'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 't'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 't', 'x'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 't', 'x', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 't', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 'x'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      " Frequent subset:  FreqItemset(items=['y', 'z'], freq=3)\n"
     ]
    }
   ],
   "source": [
    "for fset in rddfreqitemsets.toLocalIterator():\n",
    "    print ( \" Frequent subset: \", fset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function toLocalIterator() maintains one partition into the driver each time, so the driver machine should have at least the same memory as any of the worker machines in your cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
