{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We present in this unit one of the most basic, and yet powerfull and efficient, machine learning systems for building automatic classifiers for different tasks, when we have a set *multivariate objects* sampled (obtained) from some data source, and we assume that they follow certain (unknown) multivariate distributtion. The system we want to learn is a classifier, i.e a system that given one object from that data source for which we know only the values for *some of its variables* (that we call the observable attributes), we want infer the value of some other of its variables. To build such system, we will follow a supervised machine learning approach, so we assume that we have available a data set of objects from that data source with **all** the variable values known.\n",
    "\n",
    "The basic tool we use to infer the value for a unknown variable from a set of known variables is the Bayes theorem. Consider first a distribution with two variables A and B. If $ P(A,B) $ is the joint probability distribution (probability of obtaining particular values for A and B for an object sampled from the distribution), we can factorize that distribution using conditional probability in two different ways:\n",
    "\n",
    "$$ P(A,B)= P(A|B)P(B) =  P(B|A)P(A) $$\n",
    "\n",
    "So, if we **do not know** $ P(A,B) $, but we know  one of these two factorizations, we can make questions about conditional probabilities in the following way. Suppose that for an object we know the value of B, and we know (**or we have learned estimations of**) the factors $P( B | A )$, $P(A)$ and $P(B)$. Then, we have that:\n",
    "\n",
    "$$ P( A | B ) = \\frac{ P( B | A )P(A)}{P(B)}  $$\n",
    "\n",
    "But this same basic principle is equally valid if $B$ is not a single variable, but a larger set of known variables from the object:  $E_1,E_2,\\ldots,E_n$ (that we also call the **evidence**), and $A$ could also be a subset of unknown variables, although in typical Naive Bayesian models there is only one unknown variable A. So, we want to compute:\n",
    "\n",
    "$$ P( A | E_1,E_2,\\ldots,E_n ) = \\frac{ P( E_1,E_2,\\ldots,E_n | A )P(A)}{P(E_1,E_2,\\ldots,E_n)}  $$\n",
    "\n",
    "Because we are always interested on the probability of variable $A$ having a particular value $a_i$ (from its whole set of variables), observe that if we compute that question for each possible value $a_j$ for $A$, in all the expressions the denominator will be equal. Actually, from probability theory we have that the denominator is equal to the sum of all the numerators with different values for $A$:\n",
    "\n",
    "$$ P(E_1,E_2,\\ldots,E_n) = \\sum_{a_j} = P( E_1,E_2,\\ldots,E_n | A=a_j )P(A=a_j)  $$\n",
    "\n",
    "So, if we compute the numerator for all the possible values for $A$, we do not need to explicitely compute the denominator, to known which value for $A$ is more probable. So, let's turn our attention to the numerator, and let's say that our target query is *proportional* to the numerator, given that all the denominators will be equal for all the possible values of $A$:\n",
    "\n",
    "$$ P( A | E_1,E_2,\\ldots,E_n ) \\propto P( E_1,E_2,\\ldots,E_n | A )P(A) $$\n",
    "\n",
    "Now, in that factorization, to compute $  P( E_1,E_2,\\ldots,E_n | A )  $, is when we are going to use one more assumption in our model (and from this assumption comes the name *naive* for our class of bayes models). If we assume that all the $E_i$ variables are independent between them (but they depend on the value of $A$), we can factorize that expression as product of simpler conditional probability expressions:\n",
    "\n",
    "$$ P( A | E_1,E_2,\\ldots,E_n ) \\propto P(E_1|A)P(E_2|A) \\ldots P(E_n | A )P(A) $$\n",
    "\n",
    "Then, if we can learn (or estimate) the values of such factors, we can build a classifier that given the known attributes $E_i$ from an object, infers the most probable value for the unknown variable $A$. How do we learn such factors ? Assume we have a data set of $m$ objects from the data source with all the variable values known:\n",
    "\n",
    "$$ \\begin{array}{lllll}  A_{11} & E_{11} & E_{12} & \\ldots & E_{1n} \\\\\n",
    "                    \\vdots & \\vdots &  \\vdots & \\vdots & \\vdots \\\\\n",
    "                   A_{m1} & E_{m1} & E_{m2} & \\ldots & E_{mn} \n",
    "\\end{array} $$\n",
    "                   \n",
    "From this data set, we can estimate any factor  $P(E_j|A)$ and $P(A)$, although the exact expressions depend on some assumptions about the class of random variables that they represent. But the important point is that they can efficiently and *independently* be estimated computing some statistics from our sample data set of $m$ objects. This makes naive bayes classifiers the most efficient class of probabilistic models to be trained in a distributed (and centralized) computation environment. \n",
    "\n",
    "Observe that if we want to compute estimations of exact probabilities for each $ P( A=a_j | E ) $, we can still do it using the expression:\n",
    "\n",
    "$$   P( A=a_j | E ) = \\frac{ P(E_1|A=a_{j})P(E_2|A=a_{j}) \\ldots P(E_n | A=a_{j} )P(A=a_{j})  }{ \\sum_{a_{j'}} =P(E_1|A=a_{j'})P(E_2|A=a_{j'}) \\ldots P(E_n | A=a_{j'} )P(A=a_{j'})    }   $$\n",
    "\n",
    "That is, computer all the above mentioned numerators expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Our preliminary set-up code\n",
    "#\n",
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python2 not 3 if both are installed\\\\n\\\",\\n\",\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"python2\"\n",
    " \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes    \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark <pyspark.context.SparkContext object at 0x7fe7684feb00>\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)\n",
    "    \n",
    "print (spark_home, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's begin with a very simple example, where the goal is to learn a bayes model to predict the value for the first variable in each object from the known value of the other three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampledata = [ [0,1, 0, 0], [0,2, 0, 0], [0,3, 0, 0], [0,4, 0, 0],\n",
    "               [1,0, 1, 0], [1,0, 2, 0], [1,0, 3, 0], [1,0, 4, 0],\n",
    "               [2,0, 0, 1], [2,0, 0, 2], [2,0, 0, 3], [2,0, 0, 4]  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before proceeding to learn a model for querying the first variable from the others, we have to think about the class of random variables we assume they are. The spark.mllib library supports *multinomial naive Bayes* and *Bernoulli naive Bayes* variables. If our variables can only have the values {0,1}, we can choose the Bernoulli variables, but if they can have a bigger range of values {0, 1, ... , n} we must choose the multinomial variables. By default, spark assumes multinomial variables, but we can change it when we train our naive bayes model.\n",
    "\n",
    "To learn a naive bayes model, like happens with other machine learning models in spark, we have to transform our data as a set of LabeledPoint objects, where the label represents the variable for which we want to learn a prediction model and the rest of the variables represent the attributes (features) of the labeled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [1.0,0.0,0.0]), LabeledPoint(0.0, [2.0,0.0,0.0]), LabeledPoint(0.0, [3.0,0.0,0.0]), LabeledPoint(0.0, [4.0,0.0,0.0]), LabeledPoint(1.0, [0.0,1.0,0.0]), LabeledPoint(1.0, [0.0,2.0,0.0]), LabeledPoint(1.0, [0.0,3.0,0.0]), LabeledPoint(1.0, [0.0,4.0,0.0]), LabeledPoint(2.0, [0.0,0.0,1.0]), LabeledPoint(2.0, [0.0,0.0,2.0]), LabeledPoint(2.0, [0.0,0.0,3.0]), LabeledPoint(2.0, [0.0,0.0,4.0])]\n",
      "\n",
      " Model parameters: \n",
      "   labels:  [ 0.  1.  2.] \n",
      "  prior probabilities (log of):  [-1.09861229 -1.09861229 -1.09861229] \n",
      "  factor probabilities (log of):  [[-0.16705408 -2.56494936 -2.56494936]\n",
      " [-2.56494936 -0.16705408 -2.56494936]\n",
      " [-2.56494936 -2.56494936 -0.16705408]] \n",
      "\n",
      "Accuracy of the model obtained on training data :  1.0 \n",
      "\n",
      "Most Probable value for  [0.0,1.0,4.0]  :  2.0\n"
     ]
    }
   ],
   "source": [
    "bayes1RDD = sc.parallelize( sampledata ).\\\n",
    "         map( lambda sample :  LabeledPoint(sample[0], Vectors.dense( sample[1:] ) ) )\n",
    "\n",
    "print (bayes1RDD.collect())\n",
    "modelbayes1 = NaiveBayes.train(bayes1RDD)\n",
    "print ( \"\\n Model parameters: \\n\", \"  labels: \", modelbayes1.labels, \\\n",
    "\"\\n  prior probabilities (log of): \", modelbayes1.pi,   \\\n",
    "\"\\n  factor probabilities (log of): \", modelbayes1.theta, \"\\n\")\n",
    "\n",
    "# Make prediction and test accuracy on the training set.\n",
    "predictionAndLabel = bayes1RDD.map(lambda p: (modelbayes1.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pandl: pandl[0] == pandl[1]).count() / bayes1RDD.count()\n",
    "print ( \"Accuracy of the model obtained on training data : \", accuracy, \"\\n\")\n",
    "\n",
    "# Let's query (predict) the most probable value for the first variable in a new instance:\n",
    "tobj = Vectors.dense([0,1,4])\n",
    "print (\"Most Probable value for \", tobj, \" : \", modelbayes1.predict(tobj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the naive bayes model learned, the information is stored in the following members:\n",
    "\t\n",
    "- labels – list of labels.\n",
    "- pi – log of class prior probabilities, whose dimension is C, number of labels.\n",
    "- theta – log of class conditional probabilities, a matrix of C rows and D columns, where D is the number of features.\n",
    "\n",
    "We can access to this information to make any other operations with the model, apart of predicting target variable values with the predict() function. In particular, for the case of mutinomial variables, these parameters give us the following information:\n",
    "\n",
    "1. labels: the set of different values for the target variable\n",
    "2. pi: Is the set of (logarithm of) probabilities : $  [ log (P(C_k)) \\ | \\ k \\in C  ] $\n",
    "3. theta: Is the  set of (logarithm of) conditional probabilities: $ [ \\theta_{k,j} = log(P( x_j = 1 | C_k )) \\ | \\  k \\in C, j \\in D ] $. Then, with these parameters the log probability $  log(P( x_j = m \\ | \\ C_k )) $ is equal to $ m \\ \\theta_{k,j} \\ $ \n",
    "\n",
    "All these probabilities are estimations computed from our data set of points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: computing label probabilities\n",
    "\n",
    "Predicting the most probable label is good, but sometimes different labels can have almost similar probabilites, so it can be more useful to get the actual label value probabilities instead of only knowing the most probable label value.\n",
    "\n",
    "Can you implement a function for computing the probability for each possible value of the target variable ? This function exists in the java and scala versions of the naive bayes model class, but it is not available in the python wrappers (aparently). So this is an excellent programming execise, and this can be computed from the model parameters we have explained before !\n",
    "\n",
    "Take  a look here if you want to know more about how to compute the probabilities: \n",
    "> https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Compute a probability vector, with the probability of each label value for\n",
    "# the object tobj (represented as a dense or sparse spark vector with the features values)\n",
    "#\n",
    "def predictProbabilities(modelbayes,tobj):\n",
    "    \n",
    "    return probtobj\n",
    "\n",
    "#\n",
    "# Use the previous function to predict label probabilities\n",
    "# for all the points in the input RDD, and get the result\n",
    "# as a RDD that the function should return\n",
    "# Use mapPartitions instead of map to avoid creating many function\n",
    "# calls with the modelbayes parameter\n",
    "#\n",
    "def predictProbabilitiesForRDD(modelbayes,RDD):\n",
    "    \n",
    "    return probRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we are going to turn our attention on the topic of text classification. This is a basic ingredient, for example, in tradicional SPAM classifiers, or for example in news aggregation systems that need to filter huge numbers of news artices by the topics of interest for particular users. It turns out that the Naive Bayes model can give quite good results in many settings, although nowadays models based on Support Vector Machines show better accuracy in such learning tasks (and in many others). However, as we have said, training a Naive Bayes model can be done quite efficiently, so Naive Bayes models are still a good choice if we have to rebuild our learned model as new data is available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Meaning of labels:\n",
    "#\n",
    "#   label 0.0 :  soccer  text\n",
    "#   label 1.0 :  politics  text\n",
    "#   label 2.0 :  cinema    text\n",
    "#\n",
    "training_rawdocs = sc.parallelize([\n",
    "    {\"text\": \"We will give our best and we have a lot of games - the Europa League and Premier League and cups - and I think as time goes , we will be better and better .\", \"label\": 0.0},\n",
    "    {\"text\": \"Spanish league has very spensive soccer players . Messi is the best . Ronaldo sucks .\", \"label\": 0.0},\n",
    "    {\"text\": \"Champions league where the best european players can be found . Soccer matches can be dangerous for referees .\", \"label\": 0.0},\n",
    "    {\"text\": \"Spanish Soccer is all about Barcelona and Madrid \", \"label\" : 0.0},   \n",
    "    {\"text\": \"Spanish Soccer is all about politics .\", \"label\": 1.0},\n",
    "    {\"text\": \"Every spanish politician loves some spanish soccer team\", \"label\": 1.0},\n",
    "    {\"text\": \"Spanish political parties can be divided in left , centre and right parties .\", \"label\": 1.0},\n",
    "    {\"text\": \"The political power in Spain resides in Madrid .\", \"label\": 1.0},    \n",
    "    {\"text\": \"Woody Allen movies are not for every possible spectator , like you can say about Arnold Schwarzenegger movies .\", \"label\": 2.0},\n",
    "    {\"text\": \"Star Wars and Disney is a weird union but it is not George Lucas business anymore .\", \"label\": 2.0},\n",
    "    {\"text\": \"Nouvelle vague movies can be quite hard to watch .\", \"label\": 2.0},\n",
    "    {\"text\": \"François Truffaut made a movie about the novel Fahrenheit 451 .\", \"label\": 2.0}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very basic feature model for text classification\n",
    "\n",
    "For starters, let's consider a very basic model for text classification. From each text,\n",
    "we will extract the set of different words it contains, and assign a different identifier to each different word from our entire set of *corpus* documents (the set of documents we have correctly classified and we want to use them for learning a good classifier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0.0, 'words': ['we', 'better', '-', 'our', 'goes', 'the', ',', 'of', 'as', '.', 'games', 'League', 'Premier', 'have', 'and', 'cups', 'I', 'a', 'give', 'Europa', 'We', 'best', 'time', 'think', 'lot', 'be', 'will']}\n",
      "{'label': 0.0, 'words': ['players', 'league', 'sucks', 'soccer', '.', 'Ronaldo', 'spensive', 'very', 'is', 'the', 'has', 'best', 'Spanish', 'Messi']}\n",
      "{'label': 0.0, 'words': ['where', 'players', 'league', 'found', 'european', '.', 'for', 'referees', 'Champions', 'the', 'be', 'can', 'Soccer', 'matches', 'best', 'dangerous']}\n",
      "{'label': 0.0, 'words': ['Madrid', 'all', 'and', 'is', 'about', 'Soccer', 'Barcelona', 'Spanish']}\n",
      "{'label': 1.0, 'words': ['all', 'politics', '.', 'is', 'about', 'Soccer', 'Spanish']}\n",
      "{'label': 1.0, 'words': ['team', 'loves', 'politician', 'soccer', 'spanish', 'Every', 'some']}\n",
      "{'label': 1.0, 'words': ['political', 'parties', '.', 'and', 'right', 'centre', 'left', 'in', 'be', 'divided', 'can', ',', 'Spanish']}\n",
      "{'label': 1.0, 'words': ['Madrid', 'political', 'resides', 'power', '.', 'in', 'Spain', 'The']}\n",
      "{'label': 2.0, 'words': ['Schwarzenegger', 'for', '.', 'not', 'like', 'Woody', 'possible', 'you', 'about', 'Allen', 'spectator', ',', 'are', 'every', 'Arnold', 'can', 'movies', 'say']}\n",
      "{'label': 2.0, 'words': ['Disney', 'union', 'a', 'weird', 'anymore', '.', 'is', 'Wars', 'but', 'not', 'George', 'business', 'Lucas', 'it', 'Star', 'and']}\n",
      "{'label': 2.0, 'words': ['.', 'vague', 'to', 'Nouvelle', 'quite', 'be', 'can', 'movies', 'hard', 'watch']}\n",
      "{'label': 2.0, 'words': ['movie', 'a', 'novel', '451', '.', 'about', 'François', 'the', 'Fahrenheit', 'made', 'Truffaut']}\n",
      " Dictionary : \n",
      " ['where', 'team', 'give', 'goes', '451', 'like', 'are', 'vague', 'in', 'Star', 'spectator', 'of', 'think', 'best', 'players', 'but', 'political', 'Fahrenheit', 'politics', 'have', 'right', 'politician', 'François', 'novel', 'movies', 'Premier', 'business', 'movie', 'say', 'watch', 'time', 'matches', 'Disney', 'hard', 'a', 'centre', 'to', 'Arnold', 'has', 'every', 'Spanish', 'union', 'Madrid', 'weird', 'resides', 'Woody', 'divided', 'spensive', 'you', 'left', 'parties', 'lot', 'the', 'be', 'soccer', 'Spain', 'and', 'Allen', 'league', 'anymore', 'we', 'very', 'Truffaut', 'possible', 'Wars', '.', 'Lucas', 'Barcelona', 'sucks', 'loves', 'european', 'Nouvelle', 'quite', 'is', 'Every', 'games', 'as', 'cups', 'Soccer', 'Schwarzenegger', 'power', 'Ronaldo', 'The', 'I', 'found', 'referees', 'not', 'for', 'made', 'Europa', ',', 'our', 'League', 'spanish', 'dangerous', 'Messi', 'better', 'We', 'all', 'it', '-', 'Champions', 'about', 'George', 'can', 'will', 'some'] \n",
      " size:  107 \n",
      "\n",
      "[LabeledPoint(0.0, (107,[2,3,11,12,13,19,25,30,34,51,52,53,56,60,65,75,76,77,83,89,90,91,92,96,97,100,105],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(0.0, (107,[13,14,38,40,47,52,54,58,61,65,68,73,81,95],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(0.0, (107,[0,13,14,31,52,53,58,65,70,78,84,85,87,94,101,104],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(0.0, (107,[40,42,56,67,73,78,98,102],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(1.0, (107,[18,40,65,73,78,98,102],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(1.0, (107,[1,21,54,69,74,93,106],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(1.0, (107,[8,16,20,35,40,46,49,50,53,56,65,90,104],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(1.0, (107,[8,16,42,44,55,65,80,82],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(2.0, (107,[5,6,10,24,28,37,39,45,48,57,63,65,79,86,87,90,102,104],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(2.0, (107,[9,15,26,32,34,41,43,56,59,64,65,66,73,86,99,103],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(2.0, (107,[7,24,29,33,36,53,65,71,72,104],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(2.0, (107,[4,17,22,23,27,34,52,62,65,88,102],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# First, compute set of diferent words as the feature set\n",
    "# This will be expensive, as the distinct transformation is a\n",
    "# shuffle operation !\n",
    "#\n",
    "# Anyway, we locally map each document to its set of different elements\n",
    "docstofeaturesetRDD = training_rawdocs.map( lambda doc: \\\n",
    "                        { \"words\": list(set(doc[\"text\"].split())), \"label\" : doc[\"label\"] } )\n",
    "docstofeaturesetRDD.cache()\n",
    "\n",
    "for doc in docstofeaturesetRDD.collect():\n",
    "    print (doc)\n",
    "\n",
    "# \n",
    "# Beware, the EXPENSIVE (shuffle) operation comes here !\n",
    "# Do not do this with a huge collection of text documents !\n",
    "#\n",
    "dictionaryRDD = docstofeaturesetRDD.flatMap( lambda doc : doc[\"words\"] ).distinct()\n",
    "\n",
    "\n",
    "# Let's collect the collection of distinct words. NOT A GOOD IDEA if the set is too large \n",
    "# But in this basic feature model we need such complete dictionary set to be distributed\n",
    "# among partitions for being able to map documents to feature vectors\n",
    "worddict = dictionaryRDD.collect()\n",
    "sizedict = len(worddict)\n",
    "print (\" Dictionary : \\n\", worddict, \"\\n\", \"size: \",sizedict, \"\\n\")\n",
    "\n",
    "# Then, map each document to Labelled points with the feature set vector\n",
    "# We will map each partition of documents calling this function ONCE per each\n",
    "# partition, to pass the, probably large, argument wdict only once per partition\n",
    "#\n",
    "# Another more efficient solution would be to work with the dictionary stored in a RDD\n",
    "# with (key,value) format, where the keys are the words and the values are the index \n",
    "# values associated with the words.\n",
    "def mapDocsToLabelledPoints(seqofDocs,wdict,sizedict):\n",
    "    for DocFeatureSet in seqofDocs:\n",
    "        features = list()\n",
    "        values = list()\n",
    "        for w in DocFeatureSet[\"words\"]:\n",
    "            features.append(wdict.index(w))\n",
    "            values.append(1.0)\n",
    "        yield  LabeledPoint( DocFeatureSet[\"label\"] ,\\\n",
    "                            Vectors.sparse(sizedict, sorted(features), values ) )\n",
    "    \n",
    "    \n",
    "docsToLabelledPoints = docstofeaturesetRDD.mapPartitions( lambda seqofDocs : \\\n",
    "                        mapDocsToLabelledPoints(seqofDocs,worddict,sizedict) )\n",
    "\n",
    "print (docsToLabelledPoints.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's train a Naive Bayes model with our basic feature model for our data set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "\n",
      "\n",
      " Accuracy (fraction of correctly classified documents) :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes Model Classifier\n",
    "# Train and check\n",
    "model = NaiveBayes.train(docsToLabelledPoints)\n",
    "\n",
    "# Pair predicted and actual document labels\n",
    "labels_and_preds = docsToLabelledPoints.map(lambda p: \\\n",
    "                       {  \"predicted\" : model.predict(p.features), \"actual\" : p.label } )\n",
    "labels_and_preds.cache()\n",
    "\n",
    "for landp in labels_and_preds.collect():\n",
    "  print (landp)\n",
    "print (\"\\n\")\n",
    "\n",
    "# Compute fraction of sucessfully classified docs\n",
    "accuracy = 1.0 * labels_and_preds.filter(lambda x : x['predicted'] == x['actual']).count() / labels_and_preds.count()\n",
    "print (\" Accuracy (fraction of correctly classified documents) : \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting term frequency information and hashing for fast feature extraction\n",
    "\n",
    "#### The TF-IDF score for words\n",
    "The previous feature model to represent text documents can be good enough in some domains, but the fact that gives the same relevance to each single word appearing in a document can be somehow missleading in some cases.\n",
    "\n",
    "For example, some words can appear with high frequency in many documents, like articles and prepositions, but this does not mean that they provide any useful information about the topic of the document. By contrast, text documents talking about football will probably include many ocurrences of words like football, ball and player, much more frequently than in other documents of different classes. So, it seems that it makes sense to include the information about how frequent is a word with respect to the relative frequency of the world in the entire document set. One common approach to measure that information is the TF-IDF word score. This score value is computed as the product of two values. The term frequency (TF) of the word $i$ in the document $j$ (but relative to the maximum frequency of any word in the document) is computed as:\n",
    "\n",
    "$$ TF(i,j) = \\frac{f_{i,j}}{max_k f_{k,j}} $$\n",
    "\n",
    "The inverse document frequency (IDF), that is the inverse of the frequency of documents that contain the word, but in logarithmic scale, is computed as:\n",
    "\n",
    "$$ IDF(i,N) = \\log_2( N/n_i ) $$\n",
    "\n",
    "So, given a word i and a document j from our corpus of $N$ documents, the product:\n",
    "\n",
    "$$ TF\\!-\\!IDF(i,j,N) = TF(i,j) \\times IDF(i,N)  $$\n",
    "\n",
    "will be higher for such words i in a document j that appear frequently in $j$, but they are not common words in a big fraction of the documents (so they seem to give distintive/relevant information in that document). In text classification we may also substract the words that are commonly found in ANY class of documents (like articles and prepositions) before measuring the really relevant words. These common words are called stop words. However, observe that for some classes of documents an unusual high frequency of certain stop words could be signaling some specific feature about the style of the document. \n",
    "\n",
    "#### Hashing words for efficient mapping of words to feature indexes\n",
    "In order to improve the performance of our naive bayes model for text classification, there is another aspect we could improve. Observe that we work with a collection of base words (that in our case have been extracted from the corpus of documents), and that this base set is indexed to assign to each different word a different index value, that will be used to represent documents as feature vectors. But the problem is that the time needed to build the set of base words (that we also call dictionary) and then later use it in each document to find the index of each word in the document increases linearly with the size of the dictionary (at least with a dictionary of word stored as a sequence or an array of words), and if we consider big realistic dictionaries with hundreds (or thousands) of words and we have a huge collection of documents to extract their feature vectors, this can be quite expensive to compute.\n",
    "\n",
    "So, hash functions come to our rescue. How can we use them to quickly map words to integers ? Assume we have a hash function with a number of buckets enough large to encode all the different words we think that our entire corpus of documents have (or at least to encode most of the words we are interested in encoding). So, instead of keeping a list of different words and searching that list every time we need to know the index associated with a word, we simply map every word with our hash function to get the associated index with the world. Of course, we know that hash functions have **collisions**, so that means that with this representation certain words will be assigned to the same index, so their frequency counts will be added up and we will loose some information. But with good hash functions, with a large enough number of buckets, the real impact of these collisions can be minimized. Spark has an implementation of TF-IDF based scoring that uses this hashing trick to assign indexes to words.\n",
    "\n",
    "For more information about how spark uses TF-IDF based scores for words and hash functions for fast feature representation and extraction, take a look at this documentation page:\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the needed object classes, HashingTF and IDF\n",
    "#\n",
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Term frequency information (TF info represented in a SparseVector) for words in each document: \n",
      "(100,[0,1,4,11,17,22,28,31,37,39,45,46,47,59,60,62,63,67,71,73,80,87,89,92,98],[1.0,5.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "(100,[1,3,5,8,14,17,28,34,62,65,70,90,98],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0])\n",
      "(100,[3,8,17,27,28,48,62,63,73,78,89,91,94,95,98,99],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0])\n",
      "(100,[1,5,15,21,70,82,83,94],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "(100,[5,15,62,70,80,83,94],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "(100,[1,52,58,59,64,67,98],[1.0,1.0,1.0,2.0,1.0,1.0,1.0])\n",
      "(100,[1,5,13,20,31,44,52,53,62,73,93,99],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
      "(100,[6,10,21,37,44,52,62,93],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
      "(100,[1,6,21,24,27,31,34,36,41,44,57,62,73,83,88,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0])\n",
      "(100,[1,3,28,30,62,70,73,77,79,81,83,86,89,96,98],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])\n",
      "(100,[12,29,56,58,62,65,73,74,88,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "(100,[0,17,22,27,32,36,62,68,83,89,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "\n",
      "\n",
      " TF-IDF scores for words in each document: \n",
      "(100,[0,1,4,11,17,22,28,31,37,39,45,46,47,59,60,62,63,67,71,73,80,87,89,92,98],[1.46633706879,2.42753907891,1.8718021769,3.7436043538,0.955511445027,4.39901120638,1.91102289005,1.17865499634,1.46633706879,3.7436043538,1.8718021769,1.8718021769,3.7436043538,1.46633706879,1.8718021769,0.167054084663,1.46633706879,1.46633706879,3.7436043538,0.619039208406,1.46633706879,1.8718021769,0.955511445027,1.8718021769,0.773189888233])\n",
      "(100,[1,3,5,8,14,17,28,34,62,65,70,90,98],[0.485507815782,1.17865499634,1.91102289005,1.46633706879,1.8718021769,0.955511445027,0.955511445027,1.46633706879,0.501162253989,1.46633706879,0.955511445027,1.8718021769,0.773189888233])\n",
      "(100,[3,8,17,27,28,48,62,63,73,78,89,91,94,95,98,99],[1.17865499634,1.46633706879,0.955511445027,1.17865499634,0.955511445027,1.8718021769,0.334108169326,1.46633706879,1.23807841681,1.8718021769,0.955511445027,1.8718021769,1.17865499634,1.8718021769,0.773189888233,1.91102289005])\n",
      "(100,[1,5,15,21,70,82,83,94],[0.485507815782,0.955511445027,1.46633706879,1.17865499634,0.955511445027,1.8718021769,0.773189888233,1.17865499634])\n",
      "(100,[5,15,62,70,80,83,94],[0.955511445027,1.46633706879,0.167054084663,0.955511445027,1.46633706879,0.773189888233,1.17865499634])\n",
      "(100,[1,52,58,59,64,67,98],[0.485507815782,1.17865499634,1.46633706879,2.93267413759,1.8718021769,1.46633706879,0.773189888233])\n",
      "(100,[1,5,13,20,31,44,52,53,62,73,93,99],[0.485507815782,0.955511445027,3.7436043538,1.8718021769,1.17865499634,1.17865499634,1.17865499634,1.8718021769,0.167054084663,1.23807841681,1.46633706879,0.955511445027])\n",
      "(100,[6,10,21,37,44,52,62,93],[1.46633706879,1.8718021769,1.17865499634,1.46633706879,1.17865499634,2.35730999268,0.167054084663,1.46633706879])\n",
      "(100,[1,6,21,24,27,31,34,36,41,44,57,62,73,83,88,99],[0.485507815782,1.46633706879,1.17865499634,1.8718021769,1.17865499634,1.17865499634,1.46633706879,2.93267413759,1.8718021769,1.17865499634,1.8718021769,0.167054084663,0.619039208406,1.54637977647,2.93267413759,0.955511445027])\n",
      "(100,[1,3,28,30,62,70,73,77,79,81,83,86,89,96,98],[0.485507815782,1.17865499634,0.955511445027,1.8718021769,0.167054084663,1.91102289005,0.619039208406,1.8718021769,1.8718021769,1.8718021769,0.773189888233,1.8718021769,0.955511445027,2.93267413759,0.773189888233])\n",
      "(100,[12,29,56,58,62,65,73,74,88,99],[1.8718021769,1.8718021769,1.8718021769,1.46633706879,0.167054084663,1.46633706879,0.619039208406,1.8718021769,1.46633706879,0.955511445027])\n",
      "(100,[0,17,22,27,32,36,62,68,83,89,96],[1.46633706879,0.955511445027,1.46633706879,1.17865499634,1.8718021769,1.46633706879,0.167054084663,1.8718021769,0.773189888233,0.955511445027,1.46633706879])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = training_rawdocs.map(\n",
    "    lambda doc: doc[\"label\"],  # Standard Python dict access \n",
    "    preservesPartitioning=True # This is obsolete, but we want to explicitely indicate it.\n",
    ")\n",
    "\n",
    "#\n",
    "# Get the TF score of each word, but represent each word with a hash code\n",
    "# for fast mapping of words to indexes, using the HashingTF object class of Spark\n",
    "# We use tranformation HashingTF(numFeatures).transform( RDD ), where RDD must\n",
    "# be an RDD where each element is the sequence of words of a single document\n",
    "# the parameter numFeatures is the number of features we want to be able to encode,\n",
    "# so this parameter should be equal to different number of words we want to represent\n",
    "# or a smaller number if we want to compress the information with a reduced number\n",
    "# of features\n",
    "tf = HashingTF(numFeatures=100).transform(\n",
    "    training_rawdocs.map(lambda doc: doc[\"text\"].split(), \n",
    "    preservesPartitioning=True))\n",
    "\n",
    "# Make sure tf is preserved in memory, as it will be used at least two times more\n",
    "# in the following transformations\n",
    "tf.cache()\n",
    "\n",
    "# Let's check the TF-hashed representation of each document, where the information\n",
    "# for each word will be encoded as a SparseVector, with vector positions with non-zero\n",
    "# value associated with the hash value of each word\n",
    "print (\" Term frequency information (TF info represented in a SparseVector) for words in each document: \")\n",
    "tfinfo = tf.collect()\n",
    "for ti in tfinfo:\n",
    "    print (ti)\n",
    "print (\"\\n\")\n",
    "\n",
    "# Compute the TF-IDF score as the final feature value for each\n",
    "# hashed term\n",
    "# We first compute the IDF information from the TF information, and then\n",
    "# combine the TF and IDF information to get the final TFIDF score again\n",
    "# as a RDD\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# Check-it computed information\n",
    "print (\" TF-IDF scores for words in each document: \")\n",
    "tfidfinfo = tfidf.collect()\n",
    "for ti in tfidfinfo:\n",
    "    print (ti)\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Combine labels with TF-IDF feature based version of documents using zip\n",
    "# to finally build set of LabeledPoints to train NaiveBayes model\n",
    "# the RDD1.zip(RDD2) transformation creates pairs (a,b) where a comes\n",
    "# from RDD1 and b from RDD2\n",
    "training2 = labels.zip(tfidf).map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "\n",
    "# print \" training set: \", training2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's train a Naive Bayes model with this TF-IDF based feature model for our data set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 0.0, 'predicted': 0.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 1.0, 'predicted': 1.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "{'actual': 2.0, 'predicted': 2.0}\n",
      "\n",
      "\n",
      " Accuracy (fraction of correctly classified documents) :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Train and check\n",
    "model2 = NaiveBayes.train(training2)\n",
    "\n",
    "# \n",
    "labels_and_preds2 = labels.zip(model2.predict(tfidf)).map(\n",
    "    lambda x: {\"actual\": x[0], \"predicted\": float(x[1])})\n",
    "\n",
    "labels_and_preds2.cache()\n",
    "\n",
    "for landp in labels_and_preds2.collect():\n",
    "  print (landp)\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "# Compute fraction of sucessfully classified docs\n",
    "accuracy2 = 1.0 * labels_and_preds2.filter(lambda x : x['predicted'] == x['actual']).count() / labels_and_preds2.count()\n",
    "print (\" Accuracy (fraction of correctly classified documents) : \", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Small exercise**: Repeat the previous naive bayes model training example but with smaller number of features. Observe that the number of different words in a our sample document set is 107, so with the current number of features (100) we are actually merging the information for some words. How much small can the feature set be without reducing too much the accuracy of the classifier obtained ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: SPAM/HAM classification\n",
    "\n",
    "Consider a collection of messages, where some of them are SPAM messages and some others not (HAM). Train a naive bayes model using the TF-IDF score with hashing representation of words, trying different values for the numFeatures parameter. Compare the accuracy obtained with each numFeatures value. Use the collection of SPAM/HAM messages that you can find at the following repository:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "   \n",
    "You may need to apply some transformations/cleaning to get the data set in the appropiate format to be used with the spark naive bayes model learner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
