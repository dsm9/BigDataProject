{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Linear Regression Problems with Stochastic Gradient Descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to learn the basic usage of the LinearRegressionWithSGD function of the mllib module of Spark. That is, we are going to use a Stochastic Gradient Descend (SGD) algorithm to solve the problem of fitting a series of points into a linear model with the minimum possible Mean Square Error (MSE). We assume that we have a matrix with m rows and n columns X, and a column vector $Y$ with m rows. The goal is to learn a good linear model, that is, a linear model to predict the value of each $Y_i$ from the values of the row $i$ of $X$. The linear model is therefore of this kind:\n",
    "\n",
    "$$ \\hat{y_i} = \\sum_{j=1}^{n} \\beta_j x_{i,j} $$\n",
    "\n",
    "where $  \\hat{y_i} $ stands for the predicted value for $y_i$, that could not be equal to the *real* value $y_i$ if our linear model is not good enough. Of course, it may be the case that our data does not fit well (with small error) to any linear model.\n",
    "In principle, the best possible linear problem can be found exactly by solving the closed form solution of the minimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta} = \\arg \\min \\| X \\beta -Y \\|_{2} = \\arg \\min \\sum_{i=1}^m (X_i \\beta - Y_i )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closed form solution for the above problem is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta}=(X^{T}X)^{-1}X^{T}Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem is that the time complexity to solve such matrix equation is $ O(n^3+(n^2)m) $, being $n$ the number of columns of the matrix $X$ (the number of features of each point in $X$) and $m$ the number of rows of $X$ (the number of points in our data set). So, it is clear that such computation is infeasible if we are dealing with big data sets (big m) with many features per point (big n). Instead, we can try to find an **approximation** for $ \\hat{\\beta} $ using the gradient descend algorithm for the error function of the linear regression problem. The nice property is that for this problem the error function is convex, meaning that the SGD algorithm will find, with enough time, a solution that is as close to the optimal as we want. \n",
    "\n",
    "## Stochastic Gradient Descend algorithm for linear regression\n",
    "\n",
    "To understand how SGD is used to find a linear model from $X$ and $Y$, we first need to define the error function that it used to measure the error obtained with a particular linear model $\\beta$. Is this one:\n",
    "\n",
    "$$ Error(\\beta,X,Y) = \\frac{1}{2} \\sum_{i=1}^m (X_i \\beta - Y_i )^2 $$\n",
    "\n",
    "that is, the sum of  square errors. In SGD, we try to find a good model $\\beta$ by starting with an arbitrary model $\\beta_0$, and then in successive iterations modify the $n$ parameters of $\\beta$ in such a way that the error function descends a little bit in every iteration. For finding how to modify each parameter $\\beta_j$, SGD uses the partial derivative function of the error function with respect to each parameter $\\beta_j$:\n",
    "\n",
    "$$ \\frac{\\partial Error(\\beta,X,Y)}{\\partial \\beta_j} = \\sum_{i=1}^m (X_i \\beta - Y_i ) X_{i,j} $$\n",
    "\n",
    "This partial derivative function indicates us how much the error increases if we increase a little bit the parameter $\\beta_j$. So, if we modify $\\beta_j$ in the **opposite direction of this partial derivative**, we will decrease the error. Actually, SGD uses a learning parameter $\\alpha$ to modify a little bit the amount of change produced in every iteration. That is, it modifies the  value of $\\beta_j$ at the current iteration $t$ (call it $\\beta_j^t$) in this way:\n",
    "\n",
    "$$ \\beta_j^{t+1} = \\beta_j^{t} - \\alpha \\frac{\\partial Error(\\beta,X,Y)}{\\partial \\beta_j}  $$\n",
    "\n",
    "So far, we have described the classical Gradient Descend (GD) algorithm, but in the Stochastic variant (SGD), we do not use ALL the sample points at every iteration to compute the gradient (derivative of the error function). Instead, we use only a small subset of the whole set of $m$ points (or in the most extreme case only one point at every iteration). Obviously, the gradient computed in this way can be quite different from the one obtained by using all the data points, but usually after enough iterations one may converge to a solution of very good quality and spending much less computation time, because we only use a small subset of points at every iteration. So, the SGD algorithm has one more parameter, the size of the sample subset to use at every iteration to compute the gradient. This sample subset is selected uniformly at random from the whole set of $m$ points.\n",
    "\n",
    "So, a high level version of SGD could be:\n",
    "\n",
    "```python\n",
    "def computeSGD(points,maxiters,k,alpha,samplesubset):\n",
    "  iter = 1\n",
    "  beta = randomvector(k)\n",
    "  while (iter <= maxiters or Error small enough):\n",
    "     gvector = computeGradientVector( points, beta, samplesubset ) \n",
    "     betanext = beta - ( alpha * gvector )\n",
    "     iter = iter + 1\n",
    "```\n",
    "\n",
    "\n",
    "## Parallel implementation of SGD\n",
    "\n",
    "Suppose we have our data set of $m$ points stored in a RDD in spark. Can you design a high-level algorithm that performs a single iteration of the SGD algorithm ? Observe that the total error is the SUM of the individual error given by each individual point, so it should be possible to design a fairly parallel algorithm with a high degree of parallelism (at least regarding the computation of the gradient vector at each iteration). That is, each iteration of the SGD algorithm will contain:\n",
    "1. A Map operation to compute the contribution of each point to the gradient vector\n",
    "\n",
    "2. A reduce operation to sum up all these contributions to build the final gradient vector and then compute the next value of the beta vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python2 not 3 if both are installed\\\\n\\\",\\n\",\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"python2\"\n",
    "#os.environ['PYTHONPATH'] = ':'.join(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    strnumbers = line.strip().split(' ')\n",
    "    if (strnumbers[-1] == '' or strnumbers[-1].startswith(' ')): strnumbers.pop()\n",
    "    values = [float(x) for x in strnumbers]    \n",
    "    # print values\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark\n",
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "print (spark_home)\n",
    "cwd = os.getcwd()\n",
    "print (cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find a linear model for a set of data points (that you have in the file linereg-ex1.dat),  that is the following set of six points:\n",
    "\n",
    "> 21.0 2.0 3.0  \n",
    "> 27.5 2.5 4.0  \n",
    "> 17.0 1.5 2.5  \n",
    "> 31.5 0.5 6.0  \n",
    "> 34.0 8.0 2.0  \n",
    "> 47.0 4.0 7.0 \n",
    "\n",
    "where the first column is the Y value, and the two other columns are the $[X_1,X_2]$ values. This data set has been generated with this model:\n",
    "\n",
    "$$ Y= 3*X_1 + 5*X_2 $$\n",
    "\n",
    "So a good linear regression model finding algorithm should be able to find it, at least an exact algorithm, or one that converges to the good model with a sufficient number of iterations, like the SGD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: \n",
      "21.0 2.0 3.0\n",
      "27.5 2.5 4.0\n",
      "17.0 1.5 2.5\n",
      "31.5 0.5 6.0\n",
      "34.0 8.0 2.0\n",
      "47.0 4.0 7.0\n",
      "\n",
      "transformed to Labeled Point format \n",
      "(21.0,[2.0,3.0])\n",
      "(27.5,[2.5,4.0])\n",
      "(17.0,[1.5,2.5])\n",
      "(31.5,[0.5,6.0])\n",
      "(34.0,[8.0,2.0])\n",
      "(47.0,[4.0,7.0])\n"
     ]
    }
   ],
   "source": [
    "#Load points into plain text file\n",
    "data = sc.textFile(\"file:////\"+cwd+\"/linereg-ex1.dat\")\n",
    "\n",
    "# Convert data to LabeledPoint format\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "print (\"Original data: \")\n",
    "for line in data.collect():\n",
    "     print( line )\n",
    "    \n",
    "print (\"\\ntransformed to Labeled Point format \"    )\n",
    "for lpoint in parsedData.collect():\n",
    "    print ( lpoint )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for data in linereg-ex1.dat :  (weights=[2.89905270175,4.81702189966], intercept=0.0)\n",
      "\n",
      "Actual values and predicted values for each point : \n",
      "(21.0, 20.249171102470612)\n",
      "(27.5, 26.515719353002915)\n",
      "(17.0, 16.391133801767609)\n",
      "(31.5, 30.351657748825286)\n",
      "(34.0, 32.826465413296489)\n",
      "(47.0, 45.315364104599823)\n",
      "\n",
      "Mean Squared Error = 1.23952365414\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=6, step=0.1)\n",
    "\n",
    "print (\"Model for data in linereg-ex1.dat : \", model)\n",
    "\n",
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "\n",
    "print (\"\\nActual values and predicted values for each point : \")\n",
    "for yandPred in valuesAndPreds.collect():\n",
    "  print (yandPred)\n",
    "\n",
    "# Sum individual errors (with reduce) and divide by number of points to get MSE\n",
    "MSE = valuesAndPreds.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"\\nMean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with more data points\n",
    "\n",
    "Let's now try the SGD algorithm when working with more data points, this time obtained from the linear model:\n",
    "\n",
    "$$ Y= 2*X_1 + 3*X_2 + 4*X_3 $$\n",
    "\n",
    "but we are going to check different sets of data points (all coming from the same above model) increasing the number of data points from one set to the next one. In particular, we are going to try sets with 150, 350, 450, 550 and 650 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/\n",
      "['linereg-exk3err0-m150.dat', 'linereg-exk3err0-m350.dat', 'linereg-exk3err0-m450.dat', 'linereg-exk3err0-m550.dat', 'linereg-exk3err0-m650.dat']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "cwdir = os.getcwd()+\"/\"\n",
    "\n",
    "print (cwdir)\n",
    "# Get the list of all data set files starting with the prefix linereg-exk\n",
    "benchfiles = []\n",
    "for content in glob.glob(\"linereg-exk3err0-m*dat\"): # \".\" means current directory\n",
    "    benchfiles.append(content)\n",
    "    \n",
    "benchfiles.sort()\n",
    "print (benchfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linereg-exk3err0-m150.dat\n",
      "linereg-exk3err0-m350.dat\n",
      "linereg-exk3err0-m450.dat\n",
      "linereg-exk3err0-m550.dat\n",
      "linereg-exk3err0-m650.dat\n",
      "[LabeledPoint(-77.9046221664, [52.9493434086,-262.334435344,150.799999262]), LabeledPoint(-188.534740837, [-187.178048971,-79.5446078017,106.113795127]), LabeledPoint(-1351.22442219, [191.220511971,-278.22044235,-224.751029769])]\n"
     ]
    }
   ],
   "source": [
    "# Next, create RDDs for all of them\n",
    "benchRDDs = []\n",
    "for f in benchfiles:    \n",
    "    print (f)\n",
    "    benchRDDs.append( sc.textFile(\"file:////\"+cwdir+f).map(parsePoint) )\n",
    "\n",
    "# check the first 4 points of the first data file:\n",
    "print    (benchRDDs[0].collect()[0:3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to find linear models for all these sample files, but trying three different number of iterations for each one (so we will get three models for each file). We will keep the $\\alpha$ parameter (step) fixed to $0.0003$ in all the executions. It may seem a very low value, but later you can rerun this code cell with a higher value for  $\\alpha$ and check what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "finding models with iterations = 45\n",
      "model:  ->  (weights=[-1.53326094708,0.530970763892,12.2547282521], intercept=0.0)\n",
      "model:  ->  (weights=[936.74100543,-685.427285984,1827.61522078], intercept=0.0)\n",
      "model:  ->  (weights=[1148.50137361,9341.62243872,4221.74801758], intercept=0.0)\n",
      "model:  ->  (weights=[1615.36648588,-875.500921337,729.221950118], intercept=0.0)\n",
      "model:  ->  (weights=[204.409477301,52.2358383309,59.3923995929], intercept=0.0)\n",
      "\n",
      "finding models with iterations = 55\n",
      "model:  ->  (weights=[1.99978745653,2.99962952432,4.0006334281], intercept=0.0)\n",
      "model:  ->  (weights=[2.02555859472,2.97111940827,4.05899101453], intercept=0.0)\n",
      "model:  ->  (weights=[2.05666522892,3.46155484063,4.20845860064], intercept=0.0)\n",
      "model:  ->  (weights=[2.06404673869,2.96541870394,4.02489167068], intercept=0.0)\n",
      "model:  ->  (weights=[2.00222714364,3.00052894947,4.00058309423], intercept=0.0)\n",
      "\n",
      "finding models with iterations = 120\n",
      "model:  ->  (weights=[1.99978745653,2.99962952432,4.0006334281], intercept=0.0)\n",
      "model:  ->  (weights=[2.00013165408,2.99983480136,4.00031878157], intercept=0.0)\n",
      "model:  ->  (weights=[1.99990331653,2.99921248491,3.99964432332], intercept=0.0)\n",
      "model:  ->  (weights=[2.00037816869,2.99979606438,4.00014362054], intercept=0.0)\n",
      "model:  ->  (weights=[1.99943587571,2.99986612927,3.99985253103], intercept=0.0)\n",
      "Results for iterations= 45\n",
      " - Error for :  linereg-exk3err0-m150.dat  :  2656622.94194\n",
      " - Error for :  linereg-exk3err0-m350.dat  :  150080834314.0\n",
      " - Error for :  linereg-exk3err0-m450.dat  :  3.45365418353e+12\n",
      " - Error for :  linereg-exk3err0-m550.dat  :  125900091244.0\n",
      " - Error for :  linereg-exk3err0-m650.dat  :  1452477919.71\n",
      "Results for iterations= 55\n",
      " - Error for :  linereg-exk3err0-m150.dat  :  0.0179448482056\n",
      " - Error for :  linereg-exk3err0-m350.dat  :  159.635839569\n",
      " - Error for :  linereg-exk3err0-m450.dat  :  8436.44417889\n",
      " - Error for :  linereg-exk3err0-m550.dat  :  191.019138214\n",
      " - Error for :  linereg-exk3err0-m650.dat  :  0.174439564037\n",
      "Results for iterations= 120\n",
      " - Error for :  linereg-exk3err0-m150.dat  :  0.0179448482056\n",
      " - Error for :  linereg-exk3err0-m350.dat  :  0.0047003631639\n",
      " - Error for :  linereg-exk3err0-m450.dat  :  0.0245601255711\n",
      " - Error for :  linereg-exk3err0-m550.dat  :  0.00662493587148\n",
      " - Error for :  linereg-exk3err0-m650.dat  :  0.0111887528777\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "errs = {}\n",
    "for iters in [45,55,120]:\n",
    "  print ( \"\\nfinding models with iterations =\",iters)\n",
    "  errs[iters] = []  \n",
    "  models[iters] = []\n",
    "  for rdd in benchRDDs:            \n",
    "      model = LinearRegressionWithSGD.train( rdd, iterations=iters, step=0.0003) \n",
    "      print (\"model:  -> \", model)\n",
    "      models[iters].append( model )\n",
    "      valsandpreds = rdd.map(lambda p: (p.label, model.predict(p.features)))      \n",
    "      errs[iters].append( valsandpreds.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).\\\n",
    "                          reduce(lambda x, y: x + y) / valsandpreds.count() )\n",
    "\n",
    "# Show MSE of models\n",
    "for iters in [45,55,120]:\n",
    "    print (\"Results for iterations=\",iters)\n",
    "    for it,MSE in enumerate(errs[iters]):         \n",
    "       print (\" - Error for : \",benchfiles[it], \" : \",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with 45 iterations we get linear models with a hight MSE, but increasing a little bit the number of iterations the MSE improves substantially.\n",
    "Try next to execute the previous code cell but increasing a little bit the $\\alpha$ parameter, to check if you are able to get better models with the same number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a model for noisy data\n",
    "\n",
    "In the previous example, the data points follow an exact linear model, that SGD was able to find. We are now going to test our algorithm for the case where the data comes from a model that is the combination of a linear model plus some random noise added to the final value. That is:\n",
    "\n",
    "$$ Y= 2*X_1 + 3*X_2 + 4*X_3 +  \\epsilon(-2,2) $$\n",
    "\n",
    "where $\\epsilon(-2,2) $ denotes a random uniform variable in the range $ [-2,2] $. This time, SGD may have difficulties to find a model with similar errors than before. A possible way to obtain good models is to generalize a little bit our linear model to this one:\n",
    "\n",
    "$$ \\hat{y_i} = \\beta_0 + \\sum_{j=1}^{n} \\beta_j x_{i,j} $$\n",
    "\n",
    "where $\\beta_0$ represents a constant value added to the linear model, also called the intercept parameter. We can ask spark to look for a linear model with an intercept parameter by setting the optional parameter *intercept* to True when calling the train function. But observe that if the algorithm is not able to find a good value for this additional parameter, we could end up with worse models than the ones we get without the intercept parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['linereg-exk3-m010.dat', 'linereg-exk3-m050.dat', 'linereg-exk3-m150.dat', 'linereg-exk3-m1600.dat', 'linereg-exk3-m200.dat', 'linereg-exk3-m250.dat', 'linereg-exk3-m300.dat', 'linereg-exk3-m350.dat', 'linereg-exk3-m3600.dat', 'linereg-exk3-m400.dat', 'linereg-exk3-m450.dat', 'linereg-exk3-m500.dat', 'linereg-exk3-m550.dat', 'linereg-exk3-m600.dat', 'linereg-exk3-m650.dat']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all data set files starting with the prefix linereg-exk\n",
    "nbenchfiles = []\n",
    "for content in glob.glob(\"linereg-exk3-m*dat\"): # \".\" means current directory\n",
    "    nbenchfiles.append(content)\n",
    "\n",
    "nbenchfiles.sort()\n",
    "print (nbenchfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, create RDDs for all of them\n",
    "nbenchRDDs = []\n",
    "for f in nbenchfiles:    \n",
    "    nbenchRDDs.append( sc.textFile(\"file:////\"+cwdir+f).map(parsePoint) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding models for iterations= 40\n",
      "Finding models for iterations= 50\n",
      "Finding models for iterations= 120\n",
      "Results for iterations= 40\n",
      " - Error for :  linereg-exk3-m010.dat  :  2.6013886793e+15\n",
      " - Error for :  linereg-exk3-m050.dat  :  3.70474843735\n",
      " - Error for :  linereg-exk3-m150.dat  :  0.361001154739\n",
      " - Error for :  linereg-exk3-m1600.dat  :  0.341504995559\n",
      " - Error for :  linereg-exk3-m200.dat  :  0.33081562333\n",
      " - Error for :  linereg-exk3-m250.dat  :  0.334079398814\n",
      " - Error for :  linereg-exk3-m300.dat  :  0.340993882099\n",
      " - Error for :  linereg-exk3-m350.dat  :  0.34597208921\n",
      " - Error for :  linereg-exk3-m3600.dat  :  0.335061073714\n",
      " - Error for :  linereg-exk3-m400.dat  :  0.353822450877\n",
      " - Error for :  linereg-exk3-m450.dat  :  0.339986345572\n",
      " - Error for :  linereg-exk3-m500.dat  :  0.331834853466\n",
      " - Error for :  linereg-exk3-m550.dat  :  0.336203887191\n",
      " - Error for :  linereg-exk3-m600.dat  :  0.334884273319\n",
      " - Error for :  linereg-exk3-m650.dat  :  0.333334294589\n",
      "Results for iterations= 50\n",
      " - Error for :  linereg-exk3-m010.dat  :  339309468.05\n",
      " - Error for :  linereg-exk3-m050.dat  :  0.326638340828\n",
      " - Error for :  linereg-exk3-m150.dat  :  0.361001154739\n",
      " - Error for :  linereg-exk3-m1600.dat  :  0.341504995559\n",
      " - Error for :  linereg-exk3-m200.dat  :  0.33081562333\n",
      " - Error for :  linereg-exk3-m250.dat  :  0.334079398814\n",
      " - Error for :  linereg-exk3-m300.dat  :  0.340993882099\n",
      " - Error for :  linereg-exk3-m350.dat  :  0.34597208921\n",
      " - Error for :  linereg-exk3-m3600.dat  :  0.335061073714\n",
      " - Error for :  linereg-exk3-m400.dat  :  0.353822450877\n",
      " - Error for :  linereg-exk3-m450.dat  :  0.339986345572\n",
      " - Error for :  linereg-exk3-m500.dat  :  0.331834853466\n",
      " - Error for :  linereg-exk3-m550.dat  :  0.336203887191\n",
      " - Error for :  linereg-exk3-m600.dat  :  0.334884273319\n",
      " - Error for :  linereg-exk3-m650.dat  :  0.333334294589\n",
      "Results for iterations= 120\n",
      " - Error for :  linereg-exk3-m010.dat  :  0.201495568253\n",
      " - Error for :  linereg-exk3-m050.dat  :  0.326638340828\n",
      " - Error for :  linereg-exk3-m150.dat  :  0.361001154739\n",
      " - Error for :  linereg-exk3-m1600.dat  :  0.341504995559\n",
      " - Error for :  linereg-exk3-m200.dat  :  0.33081562333\n",
      " - Error for :  linereg-exk3-m250.dat  :  0.334079398814\n",
      " - Error for :  linereg-exk3-m300.dat  :  0.340993882099\n",
      " - Error for :  linereg-exk3-m350.dat  :  0.34597208921\n",
      " - Error for :  linereg-exk3-m3600.dat  :  0.335061073714\n",
      " - Error for :  linereg-exk3-m400.dat  :  0.353822450877\n",
      " - Error for :  linereg-exk3-m450.dat  :  0.339986345572\n",
      " - Error for :  linereg-exk3-m500.dat  :  0.331834853466\n",
      " - Error for :  linereg-exk3-m550.dat  :  0.336203887191\n",
      " - Error for :  linereg-exk3-m600.dat  :  0.334884273319\n",
      " - Error for :  linereg-exk3-m650.dat  :  0.333334294589\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Next, find linear models for our noisy data sets with SGD\n",
    "\n",
    "nmodels = {}\n",
    "nerrs = {}\n",
    "for iters in [40,50,120]:\n",
    "  print (\"Finding models for iterations=\",iters)\n",
    "  nerrs[iters] = []  \n",
    "  nmodels[iters] = []\n",
    "  for rdd in nbenchRDDs:\n",
    "      model = LinearRegressionWithSGD.train( rdd, iterations=iters, step=0.00022, intercept=False) \n",
    "      nmodels[iters].append( model )\n",
    "      valsandpreds = rdd.map(lambda p: (p.label, model.predict(p.features)))\n",
    "      nerrs[iters].append( valsandpreds.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).\\\n",
    "                                         reduce(lambda x, y: x + y) / valsandpreds.count() )\n",
    "\n",
    "# Show MSE of models\n",
    "for iters in [40,50,120]:\n",
    "    print (\"Results for iterations=\",iters)\n",
    "    for it,MSE in enumerate(nerrs[iters]):         \n",
    "       print (\" - Error for : \",nbenchfiles[it], \" : \",MSE)\n",
    "    \n",
    "# Next, execute again this cell but with the parameter: intercept=True in the function train    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this time we are not able to find models as good as before for our noisy data sets. You can try to rerun the previus code cell with intercept=True or with a different value for $\\alpha$, but you will see that it seems not possible to get models with smaller error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Regression with Google GDELT data\n",
    "\n",
    "Next, we are going to try to learn a linear model for predicting one the variables in google GDELT data set from some other variables in the data set. In particular we want to predict the value of the variable NumMentions, from the value of the variables: GoldsteinScale, NumSources, NumArticles and AvgTone. Check the document:\n",
    "\n",
    "http://data.gdeltproject.org/documentation/GDELT-Data_Format_Codebook.pdf\n",
    "\n",
    "If you want to know about the meaning of these variables for each event recorded in the GDELT data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapGDELTRecordsToLPoint( gdeltlines, ColumnFilter ):\n",
    "    for line in gdeltlines:\n",
    "        fields = line.split(\"\\t\")\n",
    "        values = [float(  fields[x] ) for x in ColumnFilter]\n",
    "        # yield LabeledPoint\n",
    "        yield LabeledPoint( values[0], values[1:] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED']\n",
      "[31, 30, 32, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "hlinefile = open( \"CSV.header.historical.txt\" )\n",
    "GDELT10headerline = hlinefile.readline().rstrip().split(\"\\t\")\n",
    "hlinefile.close()\n",
    "print (GDELT10headerline)\n",
    "\n",
    "# print GDELT10headerline.index( 'EventCode' )\n",
    "\n",
    "# Get the column index for the different disered attributes\n",
    "ColumnFilter = [ GDELT10headerline.index( 'NumMentions' ),\n",
    "                 GDELT10headerline.index( 'GoldsteinScale' ),\n",
    "                 GDELT10headerline.index( 'NumSources' ),\n",
    "                 GDELT10headerline.index( 'NumArticles' ),\n",
    "                 GDELT10headerline.index( 'AvgTone' ) ]\n",
    "\n",
    "print (ColumnFilter)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(9.0, [1.0,1.0,9.0,5.52631578947]), LabeledPoint(10.0, [4.0,1.0,10.0,10.9792284866]), LabeledPoint(10.0, [-5.0,1.0,10.0,10.9792284866])]\n"
     ]
    }
   ],
   "source": [
    "rddGDELT1979Points =  sc.textFile( \"GDELT10/1979.csv\" ).\\\n",
    "                    mapPartitions( lambda seq : mapGDELTRecordsToLPoint( seq, ColumnFilter ) )\n",
    "\n",
    "print (rddGDELT1979Points.take(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will modify the parameter miniBatchFraction of the SGD algorithm for finding a model in three different executions. We will try the values   $[1.0,0.5,0.01]$. Also, the smaller this fraction parameter, the smaller the number of iterations we will use, to study whether with a reduced number of iterations and sampling less points to compute the gradient really makes any impact in the quality of the model found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1gdelt = LinearRegressionWithSGD.train( rddGDELT1979Points, iterations=100, step=0.1,\n",
    "                                            intercept =True, miniBatchFraction=1.0)\n",
    "model2gdelt = LinearRegressionWithSGD.train( rddGDELT1979Points, iterations=50, step=0.1,\n",
    "                                              intercept =True, miniBatchFraction=0.5)\n",
    "model3gdelt = LinearRegressionWithSGD.train( rddGDELT1979Points, iterations=14, step=0.1,\n",
    "                                            intercept =True, miniBatchFraction=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Mean SError for GDELT model1 ( (weights=[-0.00357621422298,-0.18628543783,0.981918102254,-0.0771718479437], intercept=0.8109947394276912) ):  1.48938385509\n",
      " - Mean SError for GDELT model2 ( (weights=[-0.00272346319098,-0.100944007272,0.970513935422,-0.0960930212564], intercept=0.895487624315543) ):  1.52711032915\n",
      " - Mean SError for GDELT model3 ( (weights=[-2.36904792786,-1.84632191317,-6.1005236357,-7.3642829455], intercept=-0.9016324054961586) ):  6557.461698\n"
     ]
    }
   ],
   "source": [
    "# Print model information with their prediction errors:\n",
    "\n",
    "valsandpreds1 = rddGDELT1979Points.map(lambda p: (p.label, model1gdelt.predict(p.features)))\n",
    "MSE1 = valsandpreds1.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).\\\n",
    "                          reduce(lambda x, y: x + y) / valsandpreds1.count() \n",
    "print (\" - Mean SError for GDELT model1 (\",model1gdelt, \"): \",MSE1)\n",
    "\n",
    "valsandpreds2 = rddGDELT1979Points.map(lambda p: (p.label, model2gdelt.predict(p.features)))\n",
    "MSE2 = valsandpreds2.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).\\\n",
    "                          reduce(lambda x, y: x + y) / valsandpreds2.count() \n",
    "print (\" - Mean SError for GDELT model2 (\",model2gdelt, \"): \",MSE2)\n",
    "\n",
    "valsandpreds3 = rddGDELT1979Points.map(lambda p: (p.label, model3gdelt.predict(p.features)))\n",
    "MSE3 = valsandpreds3.map(lambda valAndPred: (valAndPred[0] - valAndPred[1])**2).\\\n",
    "                         reduce(lambda x, y: x + y) / valsandpreds3.count() \n",
    "print (\" - Mean SError for GDELT model3 (\",model3gdelt, \"): \",MSE3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
