{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We    are going to discuss in this notebook two different approaches to the problem of clustering data sets. We consider data sets of points, where each point is considered to be a $d-$ dimensional numeric vector in an euclidean space.\n",
    "Given a data set, a cluster is a subset of points of the data set that are somehow related to each other. That is, they share some similar features, or we say that globally they form a group of related/similar points. For example, consider different demografic and economic variables for the countries. Do we observe a clear division between countries when  looking at those indicators ?\n",
    "\n",
    "That we assume an euclidean space is at least necessary for our first approach, because the algorithm we use for it, the k-means algorithm, considers euclidean distances between points when measuring the Variance within each cluster produced. However, there are other variants of this algorithm that work with other distance metrics. So, if our original problem cannot be mapped, in a right way, to an euclidean space, then we shoudld consider the other variants. \n",
    "\n",
    "Both algorithms we show here are examples of **unsupervised learning** algorithms, they learn to what cluster belongs each data point without the prior knowledge of any *good* examples of points already well linked to their clusters. So, the algorithms work without practical clues about what points contain each cluster. In other words, they **discover** concepts without prior human assistance about how are the concepts they should find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark <pyspark.context.SparkContext object at 0x7f4ee04dbb38>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Our preliminary set-up code\n",
    "#\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "print (spark_home, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact (crisp) clustering with Lloyd algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the problem of *exact* clustering. We assume that there exist k clusters (groups of points), and that each point in our data set belongs to exactly one of the k clusters. We start assuming that we know the right number of clusters $k$, although this value is usually something we have to guess in real applications of clustering.\n",
    "\n",
    "For presenting the algorithm, we need to introduce some notation first:\n",
    "\n",
    "- Target set of data points: Target set of $n$-dimensional points $D=\\{X_1,X_2,\\ldots,X_m \\}$\n",
    "\n",
    "- Euclidean distance between two points: $ || x-y || = \\sqrt{ \\sum_{i = 1}^{n} (x_i-y_i)^2 } $  \n",
    "\n",
    "- Centroid of a subset Y:  $  \\frac{1}{|Y|} \\sum_{y \\in Y} y $ \n",
    "\n",
    "- Within-cluster sum of square errors (WCSSE) for a set of clusters C: \n",
    "$$ \\sum_{c \\in C} \\sum_{x \\in c} || x-center(c) ||^2 $$\n",
    "\n",
    "Then, Lloyd algorithm considers that a cluster is defined simply by its center, and that the set of points that belong to a cluster is the set of points with smaller euclidean distance to that center than to any other cluster center. We can think about the cluster center as being the mean representative point of the cluster (although it can be the case that there is no single point in our data set that is exactly equal to that cluster center).\n",
    "Then, Lloyd algorithm finds a set of k clusters with the following iterative WCSSE improving algorithm:\n",
    "\n",
    "```python\n",
    "clusters = Initial Guess for k clusters centers  \n",
    "iters = 1  \n",
    "while (clusters changed significantly  &   iters <= limit) do:  \n",
    "  # Assign closest cluster to each point based on current cluster centers   \n",
    "  for p in D:\n",
    "      assign to p its nearest cluster center                    \n",
    "  # Find best new set of cluster centers based on current centroids   \n",
    "  For each c in k: \n",
    "      cluster(c) = centroid(points in D assigned to c)  \n",
    "  iters = iters + 1 \n",
    "```\n",
    "\n",
    "From the two criterions to stop modifying the clusters, the first one refers to measuring the difference of the clusters between the previous iterations and the current one. We need to define what we mean by *changed significantly*, but usually that means that the distance between at least one pair (old_center_i,new_center_i), for some cluster i, is greater than certain threshold value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parallel implementation in the Map-Reduce framework\n",
    "\n",
    "The k-means  algorithm we have just presented is a good example of an interesting algorithm to think about an efficient implementation in the Map-Reduce framework, and in the particular Spark programming framework where we can benefit from the memory-based, resilient distributed data sets that we can define from input data or from transformations of previous iterations.\n",
    "\n",
    "First, regarding the data needed to store our current *model* for the clusters, observe that they are simply a set of k d-dimensional points (representing the centers of each cluster). So, assuming k is a small number compared with the total number of points in D (as it happens in typical applications of clustering), we are going to assume that the set of cluster centers can be mantained as shared information between all the workers in our spark application. \n",
    "\n",
    "Let's analyze how to implement the two steps performed in every iteration of Lloyd algorithm. \n",
    "- In the first step, we have to assign to each data point its closest cluster center, so in the next step we can group together all the points of each cluster to compute its centroid. This is clearly a map operation that can be performed locally with each point (provided we call a function where the set of cluster centers is packed with the function). A good way to format the ouput of such map function is to use (key,value) pairs, where the key is the cluster center and the value is a point assigned to that cluster center.\n",
    "\n",
    "- In the second step, we have to compute the centroid of each set of points assigned to the same cluster center. So, in this case this is clearly a reduceByKey operation, where the reduce operation for two points assigned to the same cluster center is simply: p1+p2 (coordinate wise addition). Once we have all the points of every cluster added, we simply divide by the number of points of each cluster to get its centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following data set of countries where we include the birth rate and death rate of every country in the data set. We will try to discover if there are any subgroups of countries, regarding their similarites in these two demographic indicators, using the k-means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Coutry information about birth rate (2nd col) and death rate (3th col):\n",
    "#\n",
    "#  The complete countries data set can be found at: data/countries_data.csv\n",
    "#\n",
    "countriesdata = [ [ 'Afghanistan', 38.6, 13.9 ], \\\n",
    "                  [ 'Armenia',  13.6  ,  9.3 ], \\\n",
    "                  [ 'India',  19.6  ,  7.3 ], \\\n",
    "                  [ 'Iran', 18  ,  5.9 ], \\\n",
    "                  [ 'Iraq', 31.5  ,  3.8 ], \\\n",
    "                  [ 'Yemen', 30  ,  6.3 ], \\\n",
    "                  [ 'Israel', 18.5  ,  5.2 ], \\\n",
    "                  [ 'Italy',  8.7  ,  10.2  ], \\\n",
    "                  [ 'Germany'  ,  8.5  ,  11.4 ], \\\n",
    "                  [  'Denmark'  ,  10.3  ,  10.3    ], \\\n",
    "                  [  'France'  ,  12.4  ,  9.2    ], \\\n",
    "                  [  'Spain'  ,  9.6  ,  9     ], \\\n",
    "                  [ 'Austria'  ,  9.4  ,  9.4  ], \\\n",
    "                  [ 'Switzerland'  ,  10.5  ,  8.1 ], \\\n",
    "                  [  'Ecuador'  ,  18.5  ,  5.1 ], \\\n",
    "                  [  'Peru'  ,  18.3  ,  6 ], \\\n",
    "                  [  'Bolivia'  ,  22.8  ,  6.5 ], \\\n",
    "                  [  'Brazil'  ,  14.5  ,  6.6, ], \\\n",
    "                  [ 'Argentina'  ,  16.6  ,  7.3 ], \\\n",
    "                  [  'Chile'  ,  13.8  ,  6    ], \\\n",
    "                  [  'Colombia'  ,  16.5  ,  5.4 ] ]\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the implementation of k-means available in spark, that incorporates an alternative way of creating the initial set of cluster centers (apart of the random method of picking uniformly at random k points to build the initial centers). But in this notebook we will not explore the alternative ways to create the initial centers, although they can have a big impact in the needed number of iterations to achieve good results. Check the paper:\n",
    "\n",
    "> B. Bahmani, B. Moseley, A. Vattani, R. Kumar and S. Vassilvitskii. *Scalable K-Means++*. In Proceedings of PVLDB 5(7): 622-633. URL: http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf (2012)\n",
    "\n",
    "If you want to know more about the specific alternative centers initialization available in the k-means implementation available in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Load specific packages for working with Kmeans clustering\n",
    "#\n",
    "# Full info about spark 1.6 k-means algorithm at:\n",
    "#\n",
    "#   http://spark.apache.org/docs/1.6.0/mllib-clustering.html#k-means\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# Evaluate the error of a point, computed with its distance to the predicted cluster center\n",
    "# (Within Cluster Sum of Squared Errors)\n",
    "def error(clusters,point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sum([x**2 for x in (point - center)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.375    9.6125]\n",
      "[ 33.36666667   8.        ]\n",
      "[ 17.71   6.13]\n",
      "\n",
      "Cluster for  Afghanistan  :  1\n",
      "Cluster for  Armenia  :  0\n",
      "Cluster for  India  :  2\n",
      "Cluster for  Iran  :  2\n",
      "Cluster for  Iraq  :  1\n",
      "Cluster for  Yemen  :  1\n",
      "Cluster for  Israel  :  2\n",
      "Cluster for  Italy  :  0\n",
      "Cluster for  Germany  :  0\n",
      "Cluster for  Denmark  :  0\n",
      "Cluster for  France  :  0\n",
      "Cluster for  Spain  :  0\n",
      "Cluster for  Austria  :  0\n",
      "Cluster for  Switzerland  :  0\n",
      "Cluster for  Ecuador  :  2\n",
      "Cluster for  Peru  :  2\n",
      "Cluster for  Bolivia  :  2\n",
      "Cluster for  Brazil  :  2\n",
      "Cluster for  Argentina  :  2\n",
      "Cluster for  Chile  :  2\n",
      "Cluster for  Colombia  :  2\n",
      "Within Cluster Sum of Squared Error = 192.020416667\n",
      "Average Square Error = 9.14382936508\n"
     ]
    }
   ],
   "source": [
    "countriesRDD = sc.parallelize( countriesdata ).map( lambda p : np.array(p[1:]) )\n",
    "countriesRDD.persist()\n",
    "\n",
    "# Cluster the data in 3 clusters\n",
    "countriesclusters1 = KMeans.train( countriesRDD, 3, maxIterations=20, \\\n",
    "                               initializationMode=\"random\")\n",
    "\n",
    "\n",
    "\n",
    "# Check the cluster assigned to each data point (user)\n",
    "# If you execute several times the clustering algorithm, given\n",
    "# the random initialization, the result obtained will be different\n",
    "# in several executions.\n",
    "# We could pick the one with smallest Error\n",
    "\n",
    "#Show clusters centers:\n",
    "for c in countriesclusters1.centers:\n",
    "    print (c)\n",
    "\n",
    "print (\"\")\n",
    "cpredictions = {}\n",
    "for country in countriesdata:\n",
    "    cpredictions[country[0]] = countriesclusters1.predict(np.array(country[1:]))\n",
    "    print (\"Cluster for \", country[0], \" : \", cpredictions[country[0]])\n",
    "\n",
    "WCSSE = countriesRDD .map(lambda point: error(countriesclusters1,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Cluster Sum of Squared Error = \" + str(WCSSE))\n",
    "print(\"Average Square Error = \"+ str(WCSSE/countriesRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we execute k-means with the countries data set, almost all the executions obtain a model with WCSSE around 190, although some of them achieve a bigger error.\n",
    "\n",
    "Because we have only two dimensions per point, we can easily plot the points of each cluster to visually inspect if the clusters learned make any sense. We can do this with a 2D plot that we make with matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAF5CAYAAADZMYNPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4eb9dc74e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "palette = itertools.cycle(['b','r','m'])\n",
    "for c in range(3):\n",
    "  cclusterx = [ x[1] for x in countriesdata if cpredictions[x[0]] == c]               \n",
    "  cclustery = [ x[2] for x in countriesdata if cpredictions[x[0]] == c]  \n",
    "  plt.scatter(cclusterx,cclustery,color=next(palette) )\n",
    "     \n",
    "\n",
    "centersX = [ x[0] for x in countriesclusters1.centers ]\n",
    "centersY = [ x[1] for x in countriesclusters1.centers ]\n",
    "# Plot the center of each cluster\n",
    "plt.scatter(centersX, centersY, marker='x', s=100, linewidths=3, color=['b','r','m'], zorder=10) \n",
    "\n",
    "plt.xlabel('birth rate')\n",
    "plt.ylabel('death rate')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now turn our attention to the topic of discovering similar users for recommender systems. In particular, let's consider a data base of users, where for each user we store the ratings given by the user to different movies. We will talk more about recommender systems later in this course, dicussing more appropriate ways of dealing with such data, where tipically many entries (actually most of the entries) of such user-product matrix will be empty, so this clustering approach may not be always usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "#\n",
    "# We have 10 users, and 10 movies: STW1, STW2, STW3, STW4, STW5, STW6\n",
    "#                                  T1, T2, T3 and BaT\n",
    "# Each entry i,j is the rating given by the user in the range [-5.0,5.0]\n",
    "# We can observe that we have 4 clear Star Wars fans (that they also like a \n",
    "# little bit Terminator movies)\n",
    "# We also have four clear Terminator fans (that they also like a little STWs movies)\n",
    "# Finally, we have two clear Breakfast at tiffannies fans (BaT), that they do not\n",
    "# like too much science-fiction movies\n",
    "\n",
    "usersandmovies = [ [3,3,3,5,5,4, 3,3,-1, -1], \\\n",
    "                   [3,3,3,5,5,4, 4,2,0, -1], \\\n",
    "                   [3,3,4,5,5,4, 4,4,1, 0], \\\n",
    "                   [4,3,3,4,5,4, 3,3,1, -1], \\\n",
    "                   [1,1,1,0,1,1, 5,4,2, -1], \\\n",
    "                   [1,2,1,0,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,0, 5,4,3, -1], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4]   ]\n",
    "\n",
    "# An important issue, that we will not cover here, is what we do when there are \n",
    "# missing entries in a data set we want to cluster with an algorithm like\n",
    "# k-means (for example, what we should put in the previous matrix for a movie j not seen\n",
    "# by an user i ?). Take a look at this blog post if you want to explore a possible solution:\n",
    "#\n",
    "#  http://blog.supplyframe.com/2013/04/30/handling-missing-data-in-k-means/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The code we use for discovering three clusters in this second data set is almost identical to the previous one, given that the input data format is almost the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster for  [3, 3, 3, 5, 5, 4, 3, 3, -1, -1]  :  1\n",
      "Cluster for  [3, 3, 3, 5, 5, 4, 4, 2, 0, -1]  :  1\n",
      "Cluster for  [3, 3, 4, 5, 5, 4, 4, 4, 1, 0]  :  1\n",
      "Cluster for  [4, 3, 3, 4, 5, 4, 3, 3, 1, -1]  :  1\n",
      "Cluster for  [1, 1, 1, 0, 1, 1, 5, 4, 2, -1]  :  2\n",
      "Cluster for  [1, 2, 1, 0, 1, 1, 4, 4, 2, -1]  :  2\n",
      "Cluster for  [1, 2, 2, 1, 1, 1, 4, 4, 2, -1]  :  2\n",
      "Cluster for  [1, 2, 2, 1, 1, 0, 5, 4, 3, -1]  :  2\n",
      "Cluster for  [-2, -3, -2, 0, -2, -1, 0, 0, -1, 4]  :  0\n",
      "Cluster for  [-2, -3, -2, 0, -2, -1, 0, 0, -1, 4]  :  0\n",
      "Within Cluster Sum of Squared Error = 14.0\n"
     ]
    }
   ],
   "source": [
    "usersandmoviesRDD = sc.parallelize( usersandmovies ).map( lambda p : np.array(p) )\n",
    "usersandmoviesRDD.persist()\n",
    "\n",
    "# Cluster the data in 3 clusters\n",
    "userclusters1 = KMeans.train( usersandmoviesRDD, 3, maxIterations=20, \\\n",
    "                               initializationMode=\"random\")\n",
    "\n",
    "# Check the cluster assigned to each data point (user)\n",
    "# If you execute several times the clustering algorithm, given\n",
    "# the random initialization, the result obtained will be different\n",
    "# in several executions.\n",
    "# We could pick the one with smallest Error\n",
    "\n",
    "for user in usersandmovies:\n",
    "    print (\"Cluster for \", user, \" : \", userclusters1.predict(np.array(user)))\n",
    "\n",
    "WCSSE = usersandmoviesRDD.map(lambda point: error(userclusters1,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Cluster Sum of Squared Error = \" + str(WCSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observe that in *almost all* the executions of k-means with this data set, we discover the three subgroups of user profiles we have intentionally introduced in this data set of user movie reviews. However, in some executions the clusters discovered differ slightly, and the WCSSE increases from 14 to 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic (soft) clustering with Expectation Maximization\n",
    "\n",
    "We are now going to tackle the problem of clustering data as assigning clusters to points following a probabilistic model. In this case, we assume that there is a probabilistic generative model with two components:\n",
    "1. There is a probability distribution among the $k$ different clusters. This probability distribution dictates which cluster generates each point obtained from the source that generated our data set.\n",
    "2. Once a cluster $i$ has been selected, the value for each dimension of the data point comes from a particular Multivariate gaussian distribution with particular mean ($d$-dimensional) vector $\\mu_i$ and covariance ($d \\times d$) matrix $\\Sigma_i$.\n",
    "\n",
    "This generation model is called a **k-Gaussian mixture model**. So, in this model we have two different sets of parameters: the probabilities that define the finite probability distribution among the $k$ clusters and the $\\mu_i$ and $\\Sigma_i$ parameters of the Multivariate gaussian distributions associated with the clusters. The conditional probability theorem tell us that the probability that a certain data point $\\overline{x}$ is obtained in our data set can be expresed as:\n",
    "\n",
    "$$ P(\\overline{x} ) = \\sum_i P(C=i) P( \\overline{x} | C=i ) $$\n",
    "\n",
    "where $ P(C=i) $ is the probability that the cluster that generated the data was $i$ and $   P( \\overline{x} | C=i ) $ is the probability to obtain $\\overline{x}$ with the multivariate gaussian distribution with parameters $\\mu_i$ and  $\\Sigma_i$. However, observe that in our clustering problem we do not know neither the cluster probabilities ($P(C=i)$) nor the gaussian parameters $\\mu_i$ and  $\\Sigma_i$ that allow us to compute $  P( \\overline{x} | C=i ) $. So, the question is:\n",
    "\n",
    "> Can we learn estimates of these $P(C=i)$ and  $\\mu_i,\\Sigma_i$ parameters from our data set ?\n",
    "\n",
    "Observe that in our data set we do not have any attributes that indicate which clusters generated each data point. So even if we assume a model where the clusters are indicated by a certain random variable, that variable **is hidden** in our data (we cannot directly observe its value in our data points). The previous conditional probability equation gives us two possible ways to discover such parameters:\n",
    "\n",
    "1. If we knew the $  P(C=i) $ parameters, we could learn the most likely values for the parameters  $\\mu_i,\\Sigma_i$ (the values for them that maximize the probabilities observed for our data points).\n",
    "\n",
    "2. If we knew the  $\\mu_i,\\Sigma_i$ parameters, we could learn the most likely values for the cluster probabilities  $  P(C=i)$.\n",
    "\n",
    "So, how can we break this deadlock situation ?\n",
    "\n",
    "### The EM algorithm for learning a k-Gaussian mixture model\n",
    "\n",
    "Given the mutual dependence between the two sets of parameters we want to learn, the approach to learn a good estimation of them is to use an iterative algorithm that although it may not always learn the best possible model, in many cases learns a good enough model (one that predicts with good accuracy the probabilities of the observed data points). This algorithm is the Expectation Maximization algorithm for a k-Gaussian mixture model. As we are going to see, this algorithm actually behaves like a probabilistic version of the previous k-means algorithm, where instead of assigning an unique cluster to each point, we define a probability distribution over cluster selection and  $\\mu_i,\\Sigma_i$  parameters for each cluster that change in every iteration until no further changes to the parameters can give a better fit to the probabilites observed. But with this  k-Gaussian mixture model, a point is never crisply assigned to a cluster, although the algorithm computes the probability that a particular cluster generated one of our data points. The EM algorithm for  learning a k-Gaussian mixture model is the following one:\n",
    "\n",
    "Initially, we generate random/pseudo-random values (or with some other method) for the parameters $  P(C=i) $ and $\\mu_i,\\Sigma_i$ .\n",
    "\n",
    "Then, we change the parameters iteratively **until they converge** repeating the two following steps:  \n",
    "1. In the E-step, we compute the expected values for the hidden indicator random variables that indicate which cluster generated each data point, given the current model parameters. That is, the expected values for probabilities $ P(C=i| \\overline{x} )  $. By bayes conditional probability rule these probabilities can be computed using our initial equation and current model parameters as:\n",
    "$$ P(C=i| \\overline{x} ) = \\frac{   P(C=i) P( \\overline{x} | C=i ) }{\\sum_j P(C=j) P( \\overline{x} | C=j ) } $$  \n",
    "Observe that $P(C=i)$ and $ P( \\overline{x} | C=i ) $ are computed with our current model parameters, as $ P( \\overline{x} | C=i ) $ is computed with the probability density function of a multivariate normal distribution with parameters  $\\mu_i,\\Sigma_i$. Check https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Density_function\n",
    "  \n",
    "2. In the M-step, we compute new parameter values for  $  P(C=i) $ and $\\mu_i,\\Sigma_i$ that maximize the likelihood of our data points, given the expected values $ P(C=i| \\overline{x} ) $ for the hidden variables  computed in the previous step:\n",
    "$$ \\mu_i = \\frac{1}{p_i} \\sum_{\\overline{x}} \\overline{x}   P(C=i| \\overline{x} )  $$ \n",
    "$$ \\Sigma_i = \\frac{1}{p_i} \\sum_{\\overline{x}}\n",
    "  P(C=i| \\overline{x} ) (\\overline{x}-\\mu_i)(\\overline{x}-\\mu_i)^T $$  \n",
    "$$ P(C=i) = p_i / N  $$ \n",
    "\n",
    "where $p_i = \\sum_{\\overline{x}} P(C=i| \\overline{x} )$  and $N$ is the number of data points in the data set.\n",
    "\n",
    "\n",
    "###  Parallel implementation in the Map-Reduce framework\n",
    "\n",
    "The implementation in a Map-Reduce framework of this algorithm follows a similar scheme to the one described for the k-means algorithm, as both algorithms perform a similar work inside of the two steps they execute in every iteration:\n",
    "\n",
    "1. In the E-step, for each point $\\overline{x}$ we have to compute the probabilites $ P(C=i| \\overline{x} )  $ associated with the hidden variable. This computation is local to each data point (for each point $\\overline{x}$  we compute its set of k $ P(C=i| \\overline{x} )  $ values using only the current parameters of the model. So, this step is totally independent for each data point, and thus it can be implemented as a pure Map operation. Observe the analogy with k-means: in k-means, in the first step we compute the most probable cluster for each point, here we compute a probability distribution over clusters for each point.\n",
    "2. In the M-step, for each cluster $i$ we compute new parameters  $  P(C=i) $ and $\\mu_i,\\Sigma_i$ collecting the values $ P(C=i| \\overline{x} )  $ computed for all the points $ \\overline{x}$. First, we can compute $\\mu_i$ using a Map-Reduce operation (with all the $  \\overline{x} P(C=i| \\overline{x} )  $ values), and then $ \\Sigma_i$ is  computed with a second Map-Reduce operation, with Map we compute the $d \\times d$ matrix $ (\\overline{x}-\\mu_i)(\\overline{x}-\\mu_i)^T $ for each point and cluster $i$, and then all such matrices with the same $i$ value are summed up in a Reduce operation to finally compute $\\Sigma_i$. Compare this with k-means: in k-means, in the second step we combined all the points of a same cluster i to compute the new centers of the clusters.\n",
    "\n",
    "\n",
    "Then, once the algorithm converges (so no better parameters can be found), we end up with a model that although is locally good, it may not necessarilly be the best global model. That is, better models could be found if we were changing at the same time both the hidden variables distributions and the clusters parameters. But there is no good method to perform such global search through the space of all parameter combinations. \n",
    "With the resulting model, observe that we can use the final parameters $  P(C=i| \\overline{x} )  $ to make a soft prediction of the cluster that generated each data point $\\overline{x}$.\n",
    "\n",
    "For a more detailed discussion of the implementation of the distributed version of this algorithm in different distributed frameworks, read the paper:\n",
    "> H. Cui, J. Wei and W Dai. *Parallel Implementation of Expectation-Maximization for Fast Convergence*. URL: https://users.ece.cmu.edu/~hengganc/archive/report/final.pdf\n",
    "\n",
    "It is worth noticing that EM based algorithms are also used for predicting values of other families of random variables in problems where we assume that our data has been generated (or can be modelled) with certain random variables where some of their parameters are unkown, but we want to learn the most probable parameters for those random variables given our data. As an another example of the use of EM based algorithms for big data applications, check for example the following paper:\n",
    "\n",
    "> Timothy Hunter, Teodor Mihai Moldovan, Matei Zaharia, Samy Merzgui, Justin Ma, Michael J. Franklin, Pieter Abbeel, Alexandre M. Bayen: Scaling the mobile millennium system in the cloud. SoCC 2011: 28\n",
    "URL: https://cs.stanford.edu/~matei/papers/2011/socc_mobile_millennium.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Load specific packages for working with Gaussian-Mixture clustering\n",
    "#\n",
    "#  More info at:\n",
    "#   http://spark.apache.org/docs/1.6.0/mllib-clustering.html#gaussian-mixture\n",
    "\n",
    "from pyspark.mllib.clustering import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the learning of a k-Gaussian mixture model with an implementation of the EM algorithm available in spark. Let's try it first with our data set of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan  soft:  array('d', [1.0, 7.051187845292559e-19, 7.051187845292559e-19])  most probable:  0\n",
      "Armenia  soft:  array('d', [4.91345524134259e-15, 0.8939218142606888, 0.10607818573930626])  most probable:  1\n",
      "India  soft:  array('d', [1.7967258440500964e-14, 0.9916321272022548, 0.00836787279772732])  most probable:  1\n",
      "Iran  soft:  array('d', [1.3300629162566005e-14, 1.3300629162566005e-14, 0.9999999999999734])  most probable:  2\n",
      "Iraq  soft:  array('d', [1.0570881106867672e-14, 0.9999999999974185, 2.5708606099069032e-12])  most probable:  1\n",
      "Yemen  soft:  array('d', [1.0, 7.051187845291895e-19, 7.057115751982228e-19])  most probable:  0\n",
      "Israel  soft:  array('d', [1.4137560418520932e-14, 1.4137560418520932e-14, 0.9999999999999717])  most probable:  2\n",
      "Italy  soft:  array('d', [1.6380994226824038e-14, 1.7069326595499638e-14, 0.9999999999999666])  most probable:  2\n",
      "Germany  soft:  array('d', [5.841111247463616e-14, 5.885378858878423e-14, 0.9999999999998828])  most probable:  2\n",
      "Denmark  soft:  array('d', [6.196290061037925e-15, 0.7246709050966031, 0.2753290949033907])  most probable:  1\n",
      "France  soft:  array('d', [1.3764717949309896e-14, 2.755515392998224e-09, 0.9999999972444708])  most probable:  2\n",
      "Spain  soft:  array('d', [1.4825420735905063e-14, 1.4825420735905063e-14, 0.9999999999999704])  most probable:  2\n",
      "Austria  soft:  array('d', [1.3192132922207254e-14, 1.3192132922207254e-14, 0.9999999999999736])  most probable:  2\n",
      "Switzerland  soft:  array('d', [1.990079955127282e-14, 1.990079955127282e-14, 0.9999999999999601])  most probable:  2\n",
      "Ecuador  soft:  array('d', [1.441077835223981e-14, 1.441077835223981e-14, 0.9999999999999711])  most probable:  2\n",
      "Peru  soft:  array('d', [1.7344685946370393e-14, 1.7344685946370393e-14, 0.9999999999999654])  most probable:  2\n",
      "Bolivia  soft:  array('d', [4.565047904097682e-15, 0.9999812473821897, 1.8752617805729283e-05])  most probable:  1\n",
      "Brazil  soft:  array('d', [8.515200471343344e-15, 8.515200471343344e-15, 0.999999999999983])  most probable:  2\n",
      "Argentina  soft:  array('d', [2.0292308647941783e-14, 2.0292308647941783e-14, 0.9999999999999594])  most probable:  2\n",
      "Chile  soft:  array('d', [3.5114427842857146e-14, 3.5114427842857146e-14, 0.9999999999999297])  most probable:  2\n",
      "Colombia  soft:  array('d', [1.3890845242459964e-14, 1.3890845242459964e-14, 0.9999999999999722])  most probable:  2\n",
      "\n",
      "\n",
      "weight =  0.0952380952381 mu =  [34.3,10.1] sigma =  [[ 18.49  16.34]\n",
      " [ 16.34  14.44]]\n",
      "weight =  0.219503433062 mu =  [20.2512972719,7.22627757859] sigma =  [[ 53.0868367  -16.25301824]\n",
      " [-16.25301824   4.98049359]]\n",
      "weight =  0.6852584717 mu =  [13.7765316307,7.55293402255] sigma =  [[ 14.26626112  -6.8526651 ]\n",
      " [ -6.8526651    3.99198457]]\n"
     ]
    }
   ],
   "source": [
    "countriesRDD = sc.parallelize( countriesdata ).map( lambda p : np.array(p[1:]) )\n",
    "countriesRDD.persist()\n",
    "\n",
    "# Learn a 3-Gaussian mixture model with EM algorithm with 20 as the maximum\n",
    "# number of iterations, but stop before if convergence is reached.\n",
    "# Remember that each iteration executes different Map-reduce operations\n",
    "# so, the less iterations the less \n",
    "gmc = GaussianMixture.train( countriesRDD , 3, maxIterations=20)\n",
    "\n",
    "# Show probability distribution over clusters for each point,\n",
    "# the probability that a certain cluster generated one of our points\n",
    "# (soft prediction)\n",
    "# This can be turned to a crisp prediction if we select for the cluster\n",
    "# of the data point the most probable cluster\n",
    "# We can also make the soft prediction for all the whole RDD with:\n",
    "bestcclustersRDD = gmc.predictSoft(countriesRDD)\n",
    "\n",
    "i = 0\n",
    "for countrypred in bestcclustersRDD.collect():\n",
    "    print ( countriesdata[i][0], \" soft: \",countrypred , \\\n",
    "    \" most probable: \", countrypred.index(max(countrypred)))\n",
    "    i = i + 1\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "# output parameters of model\n",
    "#\n",
    "#  weights[i] = probability of selecting cluster i in k-Gaussian mixture model\n",
    "#  gmc.gaussians[i].mu = mean vector for i cluster\n",
    "#  gmc.gaussians[i].sigma.toArray() = covariance matrix for i cluster\n",
    "for i in range(3):\n",
    "    print (\"weight = \", gmc.weights[i], \"mu = \", gmc.gaussians[i].mu,\n",
    "        \"sigma = \", gmc.gaussians[i].sigma.toArray())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercises\n",
    "\n",
    "Can you make a program that compares the set clusters predicted with both clustering algorithms (k-means and EM for k-Gaussian mixture) ? \n",
    "\n",
    "Can you adapt the previous matplotlib code to plot the clusters obtained with the EM algorithm (assuming we assign to each point the most probable cluster) ?\n",
    "\n",
    "Consider clustering all the countries in the file countries_data.csv but using also the variables Population, life expectancy and GDP (this last one found in the file countries_GDP.csv)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The user-movies data set\n",
    "\n",
    "Next, let's check the probabilistic model obtained with our users and movies data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array('d', [1.0, 8.298324962378164e-42, 8.298324962378164e-42])  most probable:  0\n",
      "array('d', [1.0, 8.298324962378164e-42, 8.298324962378164e-42])  most probable:  0\n",
      "array('d', [1.0, 8.298324962378045e-42, 8.298324962378045e-42])  most probable:  0\n",
      "array('d', [1.0, 8.298324962377987e-42, 8.298324962422972e-42])  most probable:  0\n",
      "array('d', [4.815055708594409e-12, 4.815055708594409e-12, 0.9999999999903699])  most probable:  2\n",
      "array('d', [4.815055708594426e-12, 4.815055708594426e-12, 0.9999999999903699])  most probable:  2\n",
      "array('d', [4.815055708594409e-12, 4.815055708594409e-12, 0.9999999999903699])  most probable:  2\n",
      "array('d', [4.8150557085944e-12, 4.8150557085944e-12, 0.9999999999903699])  most probable:  2\n",
      "array('d', [1.0771127486461517e-48, 1.0, 4.587066399685257e-44])  most probable:  1\n",
      "array('d', [1.0771127486461517e-48, 1.0, 4.587066399685257e-44])  most probable:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model (cluster the data) with expectation maximization\n",
    "# for our usersandmovies data set\n",
    "gmm = GaussianMixture.train( usersandmoviesRDD, 3, maxIterations=20)\n",
    "\n",
    "# Let's show the soft prediction for each data point\n",
    "bestclustersRDD = gmm.predictSoft(usersandmoviesRDD)\n",
    "for user in bestclustersRDD.collect():\n",
    "    print (user,\\\n",
    "    \" most probable: \", user.index(max(user)))\n",
    "\n",
    "print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, picking the most probable cluster for each data point we obtain **in many executions** the same user clusters we obtained with the most typical executions of k-means. However, observe that the probability distributions obtained over the three clusters give us more information, as they can be used to measure how similar may be two particular clusters, when considered as possible clusters for a data point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
