{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining distinct and frequent Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider basic problems in data mining, mining distinct items and frequent items  from data sets or data streams. They may seem trivial problems, but when we want to solve them with large data sets or even worst, with data streams, the traditional exact algorithms we know turn out to be not good options for solving them. Instead, we must focus on faster algorithms that provide **approximate** solutions to these problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary start-up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "# make sure pyspark tells workers to use python2 not 3 if both are installed\\n\",\n",
    "# comment the following line if you want to use the default python interpreter of\n",
    "# your spark installation, that in new versions is python 3\n",
    "#os.environ['PYSPARK_PYTHON'] = '/usr/bin/python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/spark-2.2.1-bin-hadoop2.7/\n"
     ]
    }
   ],
   "source": [
    "print ( spark_home )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print ( sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Counting the distinct elements from a data set or a data stream "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set of elements of the same type, what is the complexity of the best algorithm to find the cardinality of the set of distinct elements ? \n",
    "\n",
    "Observe that to exactly find all the distinct elements in a data set with N elements, a basic approach is based on maintaining a growing sequence that in the worst case can have the same size as the original data set, and we must compare every element of the data set with the elements of the growing sequence to check if the element is already in the growing sequence. This gives an algorithm with a worst-case time complexity of O(N²) and space complexity of O(N). We can instead think of an approach based on modifying the original sequence (eliminating duplicates of items), but this also gives an O(N²) time complexity algorithm.\n",
    "\n",
    "When we consider large data sets, as the ones we tipically will store in a spark RDD, if we are only interested on counting the number of distinct elements, and we are happy to get an **approximate value (close to the real one)**, there are approximate counting algorithms for doing this that waste much less memory and much less time, because they do not need to compare each element with each other one. These algorithms are based on the idea of computing *hash signatures* from elements and checking for *unusual* hash signatures. The basic principle is that the more distinct elements we have, the more likely is that we get one unusual hash signature from them. This is the basic idea behind the approximate probabilistic counting algorithm of Flajolet and Martin. We present here the most basic version, based on using only one single hash function. The more accurate variations of this basic algorithm are based on combining the estimates obtained from many different hash functions. \n",
    "\n",
    "Observe that in general, for working with data streams, we must in principle consider only **online algorithms**, that is, algorithms that:\n",
    "1. Process every item only once (when the item is drawn from a data stream this is sometimes a mandatory constraint, as one usually does not have time to store items in memory to later process them several times)\n",
    "2. Waste a constant amount of time per item in the input stream.\n",
    "\n",
    "The algorithm we present here for approximate distinct counting satisfies the requerirements of an on-line algorithm: it needs to process only once each element of the input sequence, and the time spent with each element can be considered constant if the maximum possible number of distinct elements cannot increase when we consider larger input sequences (the possible domain of elements is fixed in advance). However, the particular implementation we are presenting here is actually for a data set stored in an RDD, but it can be easily adapted for the case of a data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The approximate counting algorithm of Flajolet and Martin\n",
    "\n",
    "\n",
    "We next present the approximate counting algorithm of Flajolet and Martin for an input sequence (or a data set stored in any other data structure). We assume we have available a hash function that maps an input element x from the sequence to a bit string with L bits, where L should be big enough such that the range $ [0;2^L -1 ] $ of values of the hash function should be big enough to can encode all the possible distinct elements of the input sequence. This is the pseudo-code of the algorithm:\n",
    "\n",
    "```python\n",
    "for x in L:\n",
    "    bitmap[index] = 0\n",
    "for x in inputsequence:\n",
    "    # Check how much unusual is the element x\n",
    "    index = gettaillength( hashbitstring(x) ) \n",
    "    bitmap[index] = 1\n",
    "    \n",
    "R = max( {  index |  bitmap[index] == 1 }  )\n",
    "Estimate number of distinct elements as 2^R\n",
    "``` \n",
    "\n",
    "This algorithm uses the above mentioned hash function and a function, gettaillength( hash(x) ), that given the bit string obtained with the hash function, computes the position of the least significant '1'-bit in the bitstring. The idea is that the larger this position is, the most unusual we consider that the element x is. This is because if we consider that the range of our hash function is uniformly distributed, then the probability of observing a bit string with 'tail' 1 followed by k zeroes is $ 2^{-(k+1)} $ if the input element x is randomly drawn from all the possible $2^L$ elements (observe that we could consider other 'unusual' tail strings). Taking this information into account, we can compute that the expected number of **distinct elements in the sequence** that will be mapped to such tail is:\n",
    "\n",
    "$$ \\sum_{e \\ \\in \\ distinct \\ elements}  1 \\cdot 2^{-k-1} $$\n",
    "\n",
    "So, we need to draw suficiently many distinct elements to observe such long tail *at least once*. That is, we need to have at least $ 2^{k+1}$ distinct elements to have an expected value of \"1\" for the number of elements that will be mapped to such tail. So, as an **approximate inference**, if we get such tail from an element in the sequence, we deduce that the number of distinct elements is at least $2^{k+1}$.\n",
    "\n",
    "To get a better understanding about why this algorithm gives a good approximation for the distinct counting algorithm, you can check the original paper: http://algo.inria.fr/flajolet/Publications/FlMa85.pdf\n",
    "\n",
    "Or read more about the algorithm here: https://en.wikipedia.org/wiki/Flajolet%E2%80%93Martin_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here, step by step, the building blocks of a basic version of this algorithm but for the case where the data set is stored in a RDD (so we can in parallel compute the index associated with every element of the RDD) and where we use very basic hash functions but that allow us to easily explain one of the further improvements of this algorithm: using many hash functions instead of only one.\n",
    "\n",
    "Let's start by presenting a possible family of hash functions. Not any member of this family satisfies the requeriments for good hash functions, but they are good enough for understanding how this algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hash integer number x \n",
    "def hashbitstring( a, b, numbits, x ):\n",
    "    size = 2**numbits\n",
    "    val = ((a*x)+b) % size\n",
    "    return bin(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the binary hash codes obtained with three hash functions from this family  with some  values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ' -> ', '0b111', '0b1', '0b1')\n",
      "(1, ' -> ', '0b1010', '0b11', '0b1000')\n",
      "(2, ' -> ', '0b1101', '0b101', '0b1111')\n",
      "(3, ' -> ', '0b10000', '0b111', '0b10110')\n",
      "(4, ' -> ', '0b10011', '0b1001', '0b11101')\n",
      "(5, ' -> ', '0b10110', '0b1011', '0b100')\n",
      "(6, ' -> ', '0b11001', '0b1101', '0b1011')\n",
      "(7, ' -> ', '0b11100', '0b1111', '0b10010')\n",
      "(8, ' -> ', '0b11111', '0b10001', '0b11001')\n",
      "(9, ' -> ', '0b10', '0b10011', '0b0')\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print ( x, \" -> \", hashbitstring( 3, 7, 5, x ),  hashbitstring( 2, 1, 5, x ), hashbitstring( 7, 1, 5, x ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic observation is that the size of the resulting bitstring hash code should be large enough to be able to count the maximum number of distinct elements in the input data set, but if it is much larger than needed, then we can easily get big over-estimations if the estimate is based on a *single* hash function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the *unusual* feature we want to check in the resulting binary string hash code, is the length of the tail of 0s, where the longer that tail, the more unusual will be. We can compute such length from our binary strings with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gettaillength( bitstring ):\n",
    "   p = -1\n",
    "   while (bitstring[p] == '0'):\n",
    "        p -= 1\n",
    "   return -(p+1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Testing it:\n",
    "print ( gettaillength( bin(0) ), gettaillength( bin(2) ), gettaillength( bin(4) ), gettaillength( bin(8) ), gettaillength( bin(16) )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our previous test sequence, we can now compute the taillenght from each hash code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 4, 0, 1, 0, 2, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 3, 0, 1, 0, 2, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "hash1seq = [ gettaillength(hashbitstring( 3, 7, 5, x )) for x in range(10)]\n",
    "hash2seq = [ gettaillength(hashbitstring( 2, 1, 5, x )) for x in range(10)]\n",
    "hash3seq = [ gettaillength(hashbitstring( 7, 1, 5, x )) for x in range(10)]\n",
    "print (hash1seq)\n",
    "print (hash2seq)\n",
    "print (hash3seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the maximum from each of these sequences, compute the estimate 2^maximum, and order them we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 16]\n"
     ]
    }
   ],
   "source": [
    "maxvalues = sorted([2**max(hash1seq),2**max(hash2seq),2**max(hash3seq)])\n",
    "print (maxvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that from the estimations obtained, one underestimates by large (1), other overestimates by large(16), but the *median* value (8) is very close to the real value (10). So, a basic improvement over the basic algorithm based on a single hash function would be to use many different hash functions and get the median value from all the estimates obtained. \n",
    "\n",
    "Let's try a different sequence to see if we are so lucky this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq1 = [3,1,4,1,5,9,2,2,2,2,2,9,9,9,9,99,99,99,99,3,3,3,3,3,3,3,3,5,5,5,5,6,6,6,6,1,1,1,1,4,4,4,4,123,123,123,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,23,23,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 16]\n"
     ]
    }
   ],
   "source": [
    "maxvalues = sorted( [ 2**max( [ gettaillength(hashbitstring( 3, 7, 5, x )) for x in seq1 ]),\n",
    "                      2**max( [ gettaillength(hashbitstring( 2, 1, 5, x )) for x in seq1 ]),\n",
    "                      2**max( [ gettaillength(hashbitstring( 7, 1, 5, x )) for x in seq1 ]) ] )\n",
    "print (maxvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this algorithm in spark as a sequence of transformations:\n",
    "\n",
    "1. For each hash function, we first map each number to its binary string hash code, so we get an RDD for each hash function.\n",
    "2. Then, from each of these RDDs we compute their tail lengths getting new RDDs\n",
    "\n",
    "3. Finally, we have to get the maximum tail length from the RDD of each hash function (a reduce action). The estimation obtained from each hash function will be $2^{max\\_tail\\_length}$\n",
    "\n",
    "Let's do it with our example sequence seq1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Check the exact distinct count of the sequence\n",
    "#\n",
    "rdd1.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for the distributed version of our basic approximate couting algorithm, we  map each number to its tail length (for a particular hash code), and then compute the value $2^{maximun\\_tail\\_legth}$. We do this once with every hash function.\n",
    "\n",
    "As we get three different estimates, we can pick the median value of the three different maximun values as a more accurate estimation. Let's get the tail length of their hash codes from our example sequence distributed in three different RDDs and then compute with a reduce action their maximum tail lengths to finally compute the quantity  $2^{maximun\\_tail\\_legth}$ for each hash function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 16]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Map the RDD with the function that computes the tail length of the hash code of each element\n",
    "#\n",
    "rdd1hashed = rdd1.map( lambda x: gettaillength(hashbitstring(3,7,5,x)) )\n",
    "rdd2hashed = rdd1.map( lambda x: gettaillength(hashbitstring(2,1,5,x)) )\n",
    "rdd3hashed = rdd1.map( lambda x: gettaillength(hashbitstring(7,1,5,x)) )\n",
    "\n",
    "#\n",
    "# Collect back their maximun values to the driver and compute the estimates:\n",
    "#\n",
    "maxvalues = [ 2**rdd1hashed.max(), 2**rdd2hashed.max(), 2**rdd3hashed.max()  ]\n",
    "print ( sorted(maxvalues) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most accurate current variation of this probabilistic counting algorithm is the HyperLogLog counting algorithm (check https://en.wikipedia.org/wiki/HyperLogLog). Spark includes an implementation of that last algorithm, and it is available as a member function of the RDD class. Let's try it with our previous test sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10L"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute HyperLogLog probabilistic counting algorithm to approximate distinct number of elements\n",
    "rdd1.countApproxDistinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has an optional parameter (relativeSD) that controls the desired accuracy of the probabilistic counting. The default accuracy is 0.05, and we observe that with that accuracy we get the exact count of our test data set. Let's try this algorithm with 10 random sequences of 100000 integer numbers each one in the range [0,1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Exact : ', 95055, 'approx: ', 97134L)\n",
      "(' Exact : ', 95120, 'approx: ', 97767L)\n",
      "(' Exact : ', 95192, 'approx: ', 96879L)\n",
      "(' Exact : ', 95242, 'approx: ', 99828L)\n",
      "(' Exact : ', 95111, 'approx: ', 88874L)\n",
      "(' Exact : ', 95176, 'approx: ', 93131L)\n",
      "(' Exact : ', 95146, 'approx: ', 96783L)\n",
      "(' Exact : ', 95125, 'approx: ', 93585L)\n",
      "(' Exact : ', 95204, 'approx: ', 101357L)\n",
      "(' Exact : ', 95203, 'approx: ', 96229L)\n"
     ]
    }
   ],
   "source": [
    "for test in range(10):\n",
    "    rddtest = sc.parallelize( [ random.randint(0,1000000) for x in xrange(100000) ] )\n",
    "    print ( \" Exact : \", rddtest.distinct().count(), \"approx: \", rddtest.countApproxDistinct() )\n",
    "    \n",
    "# MINI-EXERCISE:\n",
    "#  Use the notebook macro %timeit -n1 -r 1 to compare the running time of the exact counting function\n",
    "#  with the one of the approximate counting function in the previous test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, approximate counting can fail to provide an exact result, but observe that the relative error is not too high. Also, if we consider the case of data streams, that cannot be stored entirely in a RDD before processing all its elements, the approach followed by the approximate counting algorithm, where each element is processed only once and it is not necessary to store it to compare it with the others, is the only one algorithmic approach to be able to approximate the number of distinct elements of the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Mining frequent items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related problem is to determine the set of items from a data set that are most frequent, where by 'most frequent' we mean those that their frequency is bigger than a threshold value $\\theta$. Observe that we cannot have more than $1/\\theta$ different items with frequency $> \\theta$ in a same sequence, so this gives us a general upper bound on how many different elements we should keep in an optimal exact algorithm for mining $\\theta$-frequent items. It is worth noticing that depending on the particular value of $\\theta$, this upper bound can be tunned a little bit, as we will discuss in some examples. The problem is that if we think about an  **exact on-line algorithm** for this problem (single pass) and constant time per item, it is known that we need memory $ \\Omega( n \\log (N/n) )  $, where $n$ is the number of different symbols in the input sequence and $N$ is its lenght. \n",
    "\n",
    "So, as with the previous problem, we are going to explain an algorithm, that it is implemented in the spark dataframe library, to **approximately count** the number of $\\theta$-frequent items. This approximate algorithm can give *false positives*, that is, elements that are not $\\theta$-frequent, but never a false negative (any element that is $\\theta$-frequent will be found). This approximate algorithm is single pass, and only wastes $O(1/\\theta) $ memory (of the order of the maximum number of different elements that can all be $\\theta$-frequent). So, its memory consumption is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the pseudo-code of the algorithm of Karp, Shenker and Papadimitriou for approximately mining items with frequency $ > \\theta $. You can find the full paper with the analysis of the algorithm in this link: https://www.cs.bgu.ac.il/~dinitz/Course/SS-12/Karp-frequent-el.pdf.\n",
    "\n",
    "```python\n",
    "  def MineFrequentItems(Sequence,theta)\n",
    "      # countDict will be a dictionary that will contain at most (1/theta) \n",
    "      # different elements from the sequence\n",
    "      countDict = {}\n",
    "      for a in Sequence:\n",
    "         if (a in countDict.keys()):\n",
    "            countDict[a] = countDict[a] + 1\n",
    "         else:\n",
    "            countDict[a] = 1\n",
    "            if ( size(countDict.keys()) > 1/theta ):\n",
    "               # Eliminate occurences of elements as not all of them\n",
    "               # can be theta-frequent\n",
    "               for ap in countDict.keys():\n",
    "                 countDict[ap] = countDict[ap] - 1\n",
    "                 # eliminate elements with 0 ocurrences\n",
    "                 # at least a will be eliminated\n",
    "                 if (countDict[ap] == 0): del(countDict[ap])\n",
    "      # Return the remaining elements in countDict as the theta-frequent\n",
    "      # possibly with some false positives\n",
    "      return countDict.keys()\n",
    "```\n",
    "\n",
    "The algorithm, as presented here, is appropriate for a sequential, stream version of it. For the case of working with the distributed data frames of spark, the version used by spark works with many counting dictionaries (one per partition of the RDD), and then combines them with a merge operation. \n",
    "\n",
    "Let's check how the algorithm works with some small examples:\n",
    "\n",
    "- Assume the following sequence with 9 elements, and the value $\\theta=1/2$:\n",
    "\n",
    "```python \n",
    "   [ 1, 2, 3, 4, 5, 1, 1, 1, 1]\n",
    "```\n",
    "For this particular value of $\\theta$, instead of the general upper bound (the set of  $1/2$-frequent elements cannot contain more than 2 elements), we can can use a better one: the set cannot contain more than 1 element. That is, every time we have two elements in the set we eliminate one occurrence of every simbol in the sequence. Working that way, we end the algorithm having only the element 1 with a number of occurrences equal to 3. Observe that the element \"1\" appears 5 times (> 9 * 1/2), so the final set contains the right answer.\n",
    "\n",
    "- In the second example, we consider a different value for $\\theta$ (1/2.5), and the following sequence with 10 elements:\n",
    "\n",
    "```python \n",
    "   [ 1, 2, 3, 1, 2, 3, 2, 3, 2, 3]\n",
    "```\n",
    "\n",
    "In this case, the general upper bound is the tightest one, as we can have 2 elements with frequency 5 (> 1/2.5 * 10) but no 3 such elements. With this upper bound, we end the algorithm having only the elements 2 and 3 with a number of occurrences equal to 2. Observe that such elements have a real frequency of 4, so they do not satisfy having frequency 5 (although they almost satisfy it !!). So, in this case, the final set contains elements that are not really part of the correct answer (actually, there are no elements with frequency $> 4$). \n",
    "\n",
    "- In the third example, we consider a value of $\\theta$ equal to $1/3$, and the same sequence as in the previous example:\n",
    "\n",
    "```python \n",
    "   [ 1, 2, 3, 1, 2, 3, 2, 3, 2, 3]\n",
    "```\n",
    "\n",
    "In this case, the general upper bound can be refined as in the first example: the set of $1/3$-frequent elements cannot contain more than 2 elements. If we consider this more refined upper bound, we end the algorithm having only elements 2 and 3 with a number of occurrences equal to 2. In this case, the answer is correct, as elements 2 and 3 have a real frequency of 4 (that is bigger than  1/3 * 10). \n",
    "\n",
    "However, observe that it is safe to use always the general upper bound (in any case, the set cannot contain more than $1/\\theta$ elements), for a general value of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's test this algorithm working a little bit with a data set obtained from the city of Chicago, that contains crime information from 2006 to 2016 (except murders). The original format is a CSV file, and we will have to convert it to dataframes if we want to execute the frequent items algorithm of the data frames library of spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddchicagocrimes = sc.textFile(\"./ChicagoCrimes_2006to2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3773539"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddchicagocrimes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " u'10479397,HZ218150,04/08/2016 01:30:00 PM,0000X W TERMINAL ST,0484,BATTERY,PRO EMP HANDS NO/MIN INJURY,AIRPORT TERMINAL UPPER LEVEL - SECURE AREA,true,false,1653,016,41,76,08B,1101811,1934419,2016,05/06/2016 03:48:54 PM,41.976762981,-87.900983721,\"(41.976762981, -87.900983721)\"',\n",
       " u'10480217,HZ219687,04/09/2016 11:00:00 AM,011XX S DELANO CT W,0810,THEFT,OVER $500,RESIDENCE,false,false,0123,001,2,32,06,1175106,1895291,2016,05/06/2016 03:48:54 PM,41.868062668,-87.632621013,\"(41.868062668, -87.632621013)\"',\n",
       " u'10484708,HZ222956,04/11/2016 02:49:00 PM,012XX S WABASH AVE,1150,DECEPTIVE PRACTICE,CREDIT CARD FRAUD,OTHER,false,false,0131,001,2,33,11,,,2016,05/06/2016 03:48:54 PM,,,',\n",
       " u'3732,HM520797,08/05/2006 03:10:00 AM,014XX N MAYFIELD AVE,0110,HOMICIDE,FIRST DEGREE MURDER,STREET,true,false,2531,025,29,25,01A,1136757,1908976,2006,08/17/2015 03:03:40 PM,41.906388799,-87.773080931,\"(41.906388799, -87.773080931)\"']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddchicagocrimes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headerChicagoCrimes = rddchicagocrimes.first()\n",
    "headerChicagoCrimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(ID,StringType,true),\n",
       " StructField(Case Number,StringType,true),\n",
       " StructField(Date,StringType,true),\n",
       " StructField(Block,StringType,true),\n",
       " StructField(IUCR,StringType,true),\n",
       " StructField(Primary Type,StringType,true),\n",
       " StructField(Description,StringType,true),\n",
       " StructField(Location Description,StringType,true),\n",
       " StructField(Arrest,StringType,true),\n",
       " StructField(Domestic,StringType,true),\n",
       " StructField(Beat,StringType,true),\n",
       " StructField(District,StringType,true),\n",
       " StructField(Ward,StringType,true),\n",
       " StructField(Community Area,StringType,true),\n",
       " StructField(FBI Code,StringType,true),\n",
       " StructField(X Coordinate,StringType,true),\n",
       " StructField(Y Coordinate,StringType,true),\n",
       " StructField(Year,StringType,true),\n",
       " StructField(Updated On,StringType,true),\n",
       " StructField(Latitude,StringType,true),\n",
       " StructField(Longitude,StringType,true),\n",
       " StructField(Location,StringType,true)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldsChicagoCrimes = [StructField(field_name, StringType(), True) for field_name in headerChicagoCrimes.split(',')]\n",
    "fieldsChicagoCrimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many fields that are not obviously of string type, but for the *query* we want to execute now we do not need to consider the right data types for every column. Check for example this tutorial page:\n",
    "\n",
    "  http://www.nodalpoint.com/spark-data-frames-from-csv-files-handling-headers-column-types/\n",
    "  \n",
    "if you want to know more about converting CSV to spark data frames with correct data types for each column\n",
    "\n",
    "NOTE: In spark 2.0 there is a built-in CSV reader, that in principle should make thinks easier. But converting CSV to dataframes is a good data cleaning/processing exercise to try with spark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaChicagoCrimes = StructType(fieldsChicagoCrimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the csv python module to parse the lines of the csv loaded into the RDD. We are going to map each partition with a single function call, using the mapPartitions function instead of the usual map function. Doing it in this way we gain some efficiency, because only one csv.reader object will be created and used to iterate over all the CSV lines of each partition, instead of creating one CSV reader object for each single CSV line. The following function is the one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseCSVPartition( csvseq ):\n",
    "    import csv\n",
    "    reader = csv.reader(csvseq)  # creates the reader object for the iterable of CSV records\n",
    "    for row in reader:   # iterates the rows of the file in orders\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chicagocrimesonlydataRDD = rddchicagocrimes.filter( lambda l: not l.startswith('ID,Case Number' ) ).mapPartitions( parseCSVPartition )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10479397',\n",
       "  'HZ218150',\n",
       "  '04/08/2016 01:30:00 PM',\n",
       "  '0000X W TERMINAL ST',\n",
       "  '0484',\n",
       "  'BATTERY',\n",
       "  'PRO EMP HANDS NO/MIN INJURY',\n",
       "  'AIRPORT TERMINAL UPPER LEVEL - SECURE AREA',\n",
       "  'true',\n",
       "  'false',\n",
       "  '1653',\n",
       "  '016',\n",
       "  '41',\n",
       "  '76',\n",
       "  '08B',\n",
       "  '1101811',\n",
       "  '1934419',\n",
       "  '2016',\n",
       "  '05/06/2016 03:48:54 PM',\n",
       "  '41.976762981',\n",
       "  '-87.900983721',\n",
       "  '(41.976762981, -87.900983721)'],\n",
       " ['10480217',\n",
       "  'HZ219687',\n",
       "  '04/09/2016 11:00:00 AM',\n",
       "  '011XX S DELANO CT W',\n",
       "  '0810',\n",
       "  'THEFT',\n",
       "  'OVER $500',\n",
       "  'RESIDENCE',\n",
       "  'false',\n",
       "  'false',\n",
       "  '0123',\n",
       "  '001',\n",
       "  '2',\n",
       "  '32',\n",
       "  '06',\n",
       "  '1175106',\n",
       "  '1895291',\n",
       "  '2016',\n",
       "  '05/06/2016 03:48:54 PM',\n",
       "  '41.868062668',\n",
       "  '-87.632621013',\n",
       "  '(41.868062668, -87.632621013)']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicagocrimesonlydataRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data set without the header line, we can finally convert it to data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=u'10479397', Case Number=u'HZ218150', Date=u'04/08/2016 01:30:00 PM', Block=u'0000X W TERMINAL ST', IUCR=u'0484', Primary Type=u'BATTERY', Description=u'PRO EMP HANDS NO/MIN INJURY', Location Description=u'AIRPORT TERMINAL UPPER LEVEL - SECURE AREA', Arrest=u'true', Domestic=u'false', Beat=u'1653', District=u'016', Ward=u'41', Community Area=u'76', FBI Code=u'08B', X Coordinate=u'1101811', Y Coordinate=u'1934419', Year=u'2016', Updated On=u'05/06/2016 03:48:54 PM', Latitude=u'41.976762981', Longitude=u'-87.900983721', Location=u'(41.976762981, -87.900983721)'),\n",
       " Row(ID=u'10480217', Case Number=u'HZ219687', Date=u'04/09/2016 11:00:00 AM', Block=u'011XX S DELANO CT W', IUCR=u'0810', Primary Type=u'THEFT', Description=u'OVER $500', Location Description=u'RESIDENCE', Arrest=u'false', Domestic=u'false', Beat=u'0123', District=u'001', Ward=u'2', Community Area=u'32', FBI Code=u'06', X Coordinate=u'1175106', Y Coordinate=u'1895291', Year=u'2016', Updated On=u'05/06/2016 03:48:54 PM', Latitude=u'41.868062668', Longitude=u'-87.632621013', Location=u'(41.868062668, -87.632621013)')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicagocrimesDF = sqlContext.createDataFrame( chicagocrimesonlydataRDD, schemaChicagoCrimes )\n",
    "chicagocrimesDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can find frequent items for the columns we want. For example, for columns 'Primary Type' and 'Community Area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq01rdd = chicagocrimesDF.freqItems([\"Primary Type\", \"Community Area\"], 0.1)\n",
    "freq03rdd = chicagocrimesDF.freqItems([\"Primary Type\", \"Community Area\"], 0.3)\n",
    "freq05rdd = chicagocrimesDF.freqItems([\"Primary Type\", \"Community Area\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Primary Type_freqItems=[u'DECEPTIVE PRACTICE', u'MOTOR VEHICLE THEFT', u'BURGLARY', u'THEFT', u'OTHER OFFENSE', u'NARCOTICS', u'BATTERY', u'CRIMINAL DAMAGE', u'HOMICIDE', u'ASSAULT'], Community Area_freqItems=[u'8', u'14', u'49', u'43', u'16', u'25'])\n",
      "Row(Primary Type_freqItems=[u'THEFT', u'BATTERY', u'HOMICIDE'], Community Area_freqItems=[u'49', u'31'])\n",
      "Row(Primary Type_freqItems=[u'BATTERY', u'THEFT'], Community Area_freqItems=[u'27', u'49'])\n"
     ]
    }
   ],
   "source": [
    "print (freq01rdd.collect()[0])\n",
    "print (freq03rdd.collect()[0])\n",
    "print (freq05rdd.collect()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that results obtained for different threshold frequencies are not necessarily consistent. Why ? Check the actual code of the estimation algorithm, and remember that it is an *approximation* algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about transformations and actions with spark dataframes, check for example this blog page at databricks:\n",
    "  \n",
    "     https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Find the distinct elements and most frequent elements (with frequencies $> 0.15$, $> 0.30$ and $> 0.50$) for the attribute column 'EventCode' from the GDELT data set GDELT10/1979.csv\n",
    "\n",
    "> Check the web page http://www.gdeltproject.org/ if you want to know more about the GDELT project, or\n",
    "> here: https://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations if you want to know more \n",
    "> about the encoding of information in the event data sets. The concrete data set (1979.csv), can be downloaded from the URL: http://data.gdeltproject.org/events/1979.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED'], '\\n')\n"
     ]
    }
   ],
   "source": [
    "# GDEL1.0 files separate columns with tabulators\n",
    "def parseGDELT10line( line ):\n",
    "    return line.split(\"\\t\")\n",
    "\n",
    "hlinefile = open( \"CSV.header.historical.txt\")\n",
    "GDELT10headerline = hlinefile.readline().rstrip().split(\"\\t\")\n",
    "hlinefile.close()\n",
    "print ( GDELT10headerline, \"\\n\")\n",
    "# Get the column index for the 'EventCode' column\n",
    "eventcodecol = GDELT10headerline.index( 'EventCode' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  PUT YOUR SOLUTION CODE IN THIS CELL AND IN THE FOLLOWING ONES\n",
    "#    Use the function parseGDEL10line to transform each line (GDELT record) to a list of strings\n",
    "\n",
    "# Setup a RDD with the CSV file GDELT10/1979.csv loading each line as a python list with the fields of the line\n",
    "rddGDELT1979 = \n",
    "\n",
    "# filter the eventcode column to a new RDD with only that column\n",
    "rddEventCodes = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show total number of records and number of different event codes first with the exact algorithm\n",
    "# Compare their runing times with the \n",
    "# notebook macro %timeit -n1 -r 1  pythonsentence\n",
    "print \" Total : \"\n",
    "%timeit -n 1 -r 1 print \" Different event codes : \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, show the approximate number of distinc event codes obtained with the\n",
    "# approximation algorithm:\n",
    "\n",
    "%timeit -n 1 -r 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next create the dataframes version of the RDD (all with StringType) for using the freqItems() function\n",
    "fieldsGDELT10 = \n",
    "schemaGDELT10 = \n",
    "# Create the DF version of  rddGDELT1979  with the schema information:\n",
    "GDELT10_1979_DF = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the freqItems function over the GDELT10_1979_DF dataframes RDD with freq > 0.15, > 0.3 and > 0.5\n",
    "gdeltfreq015rdd = \n",
    "gdeltfreq030rdd = \n",
    "gdeltfreq050rdd = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the resulting RDDs with frequent items:\n",
    "print \n",
    "print \n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
