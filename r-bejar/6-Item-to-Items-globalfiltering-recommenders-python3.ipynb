{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-to-item based collaborative filtering recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will consider how to build recommender systems based on item-to-item (where items are the products of the on-line store) collaborative filtering, where the approach to recommend new items to an user is based on finding SIMILAR items to the ones already bought by the user, or similar to the ones we know the user likes. This approach is a good one when the user-item matrix represents which products were bought by each user, but the typical user row vector is sparse (there are few purchases with respect to the total number of items of the on-line store). This item-centered approach allows us to compute recommendations for any single item bought by any user, so even in the extreme case where the user has only bought one item, the system will be able to compute good recommendations if the total number of purchases of the on-line store is high enough, as recommendations depend on the number of new items (non bought ones) that are similar enough to any already bought.\n",
    "\n",
    "We can also consider not only wich items the user bought, but also wich items he has browsed in the online store frequently, or any other source of implicit knowledge about the interests of the user.\n",
    "\n",
    "In any case, the main issue here will be **how to compare items of the on-line store** and how to predict wich ones will be more interesting for the user, given his current set of purchases or interests. So, the difference with the approach followed in the case of the recommender systems with latent factors is that here, although we also perform global filtering because we consider the whole set of users and items to build the system, we compute explicitely similarity measures between any pair of products, instead of decomposing users and products as vectors of latent factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary start-up code for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "# from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print (spark_home)\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "print (sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing items based on global comparison of common costumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first step towards discovering (mining) similar items is how to measure the similarity between two items in the store catalog. If we assume that the information we have for every item is the set of users that bought that item, the global filtering approach is based on comparing ALL the costumers of the store, checking how many costumers have bought both the two items we want to compare. The idea is that the similarity measure should be higher when the two items have a higher number of common costumers. So, a very direct way of measuring the distance would be to count the number of common costumers, and normalize it by the total number of costumers of the store.\n",
    "\n",
    "However, when we consider user-item matrices that incorporate more complex information about user-item pairs, for example the satisfaction degree of the costumer with the item, it is better to consider other kinds of similarity measures. One widely used distance measure is the cosine distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For working on item-to-item based collaborative filtering, we must work with the columns of the user-item matrix that contains wich items have been bough by each user, the same matrix that we used with global filtering based on UV decomposition.\n",
    "\n",
    "In this section we will assume that items are already given as vectors, representing their corresponding column vectors in the user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Compute the cosine distance between vectors vec1 and vec2, represented\n",
    "#  as dense lists: with all the elements (non-zero and zero) values present\n",
    "#\n",
    "def CosineDistance( vec1, vec2 ):\n",
    "    # dot product of vec1 and vec2\n",
    "    dot = 0.0\n",
    "    v1rs = 0.0\n",
    "    v2rs = 0.0\n",
    "    for i in range(len(vec1)):\n",
    "        dot += (vec1[i]*vec2[i]) \n",
    "        v1rs += (vec1[i]*vec1[i])\n",
    "        v2rs += (vec2[i]*vec2[i])\n",
    "    v1rs = math.sqrt(v1rs)\n",
    "    v2rs = math.sqrt(v2rs)\n",
    "    return dot/(v1rs*v2rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's think about implementing the cosine distance but considering the vectors as sparse vector data types of the mllib spark library\n",
    "\n",
    "By considering a sparse representation we can work with bigger vectors (if many entries are blank as it should be the case for an online store where many items will be bought only by a small fraction of the total number of costumers)\n",
    "\n",
    "Check https://spark.apache.org/docs/1.6.0/mllib-data-types.html for basic information about spark mllib data types for matrices and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Implement cosine distance working with the sparse vector data type of spark mllib library\n",
    "# Observe that we are still assuming that single vectors can fit into the main memory of a single node when using\n",
    "# this function for computing with spark\n",
    "#\n",
    "def CosineDistance_sparkSparseVectors( svec1, svec2 ):\n",
    "    #\n",
    "    # INSERT CODE HERE\n",
    "    #\n",
    "    \n",
    "    return dot/(v1rs*v2rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider for example the following set of 5 item vectors, each one indicating wich costumers, from a total set of 4 costumers, have bought (1) or have not bought (0) the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemvectors = [ [1,1,0,0], [0,1,0,1], [0,1,0,0], [1,0,1,0], [0,1,0,1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.4999999999999999\n",
      "Cosine distance between  0 and 2 : 0.7071067811865475\n",
      "Cosine distance between  0 and 3 : 0.4999999999999999\n",
      "Cosine distance between  0 and 4 : 0.4999999999999999\n",
      "Cosine distance between  1 and 2 : 0.7071067811865475\n",
      "Cosine distance between  1 and 3 : 0.0\n",
      "Cosine distance between  1 and 4 : 0.9999999999999998\n",
      "Cosine distance between  2 and 3 : 0.0\n",
      "Cosine distance between  2 and 4 : 0.7071067811865475\n",
      "Cosine distance between  3 and 4 : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the similarity between any pair of items:\n",
    "\n",
    "for i1, vec1 in enumerate(itemvectors):\n",
    "    for i2, vec2 in enumerate(itemvectors):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity value provided by the Cosine distance is also useful in case our item vectors contain values in a range that includes negative values. This is the case where negative values mean *negative* ratings, positive values mean positive ratings and 0 mean a neutral rating (or no rating at all). For example, consider the following modified example of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemvectors2 = [ [-3,-3,-3,-2], [-2,-2,-2,-1], [1,1,1,1], [2,2,2,2], [3,3,3,3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.996270962773436\n",
      "Cosine distance between  0 and 2 : -0.987829161147262\n",
      "Cosine distance between  0 and 3 : -0.987829161147262\n",
      "Cosine distance between  0 and 4 : -0.987829161147262\n",
      "Cosine distance between  1 and 2 : -0.9707253433941511\n",
      "Cosine distance between  1 and 3 : -0.9707253433941511\n",
      "Cosine distance between  1 and 4 : -0.9707253433941511\n",
      "Cosine distance between  2 and 3 : 1.0\n",
      "Cosine distance between  2 and 4 : 1.0\n",
      "Cosine distance between  3 and 4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the similarity between any pair of items:\n",
    "\n",
    "for i1, vec1 in enumerate(itemvectors2):\n",
    "    for i2, vec2 in enumerate(itemvectors2):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that in this case, the cosine distance ranges from -1 to 1, where -1 means *totally opposite vectors*, and 1 means totally aligned vectors, without giving relevance to the magnitude of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining similar items (products) based on item-to-item global filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the approach for recommending products to the Amazon costumers presented in the paper:\n",
    "\n",
    "> Greg Linden, Brent Smith, and Jeremy York. *Amazon.com recommendations: Item-to-Item Collaborative Filtering*. In IEEE INTERNET COMPUTING. 2003\n",
    "\n",
    "(Don't think that in that paper you will find exact final algorithms, only the overall idea). The approach is the one we have explained before, use some similarity measure between products to recommend relevant products, with respect to ones already bought by the user. We do not know what are the current similarity measures used by Amazon, but we will use the cosine distance in this notebook to develop our recommender system.  \n",
    "\n",
    "The main building block of the Amazon recomender system is their algorithm to compute similarity between any pair of items in their on-line catalog. This is the pseudo-code of the mentioned Amazon item-to-item similarity mining algorithm:\n",
    "\n",
    "```python\n",
    "def computeSimilarityBetweenProducts( I ):\n",
    " for each item i1 in product catalog I:\n",
    "     for each customer C who purchased i1:\n",
    "         for each item i2 purchased by customer C:\n",
    "            record that *a customer* (C) purchased i1 and i2:\n",
    "                    store that (i1,i2) were purchased by a same user\n",
    "     for each item i2 in product catalog I:\n",
    "        compute the similarity between i1 and i2\n",
    "```\n",
    "\n",
    "This algorithm can be though as an **off-line** algorithm, the similarity between products should be computed as a background process, and only be recomputed when there are a significant number of changes in the purchases database. What is the worst-case and real complexity of this algorithm ? Consider the following observations (where $M$ is the number of costumers and $N$ the number of items of the on-line store:\n",
    "\n",
    "1. The worst-case complexity is $ O(N^{2}  M)$. This is the case when almost any user has bought any item of the store (this can be considered a very unrealistic scenario).  \n",
    "2. However, if we assume that many costumers have very few purchases (let's say a constant number), the real complexity is more closer to $O(N \\ M)$.\n",
    "3. The complexity can be further reduced if we only consider a sample (subset) of costumers to compute the similarity between products (for example, users that buy best-selling products so they will tipically score in many item-to-item similarity values). Of course, this will produce an approximation of the real similarity values between products. \n",
    "\n",
    "For computing the similarity, we can use the cosine distance, or any other similarity measure we think is good for our application domain. Observe that in some sense, the similarity between a pair of products (i1,i2) will be *a number* proportional to the total number of customers that purchased both i1 and i2, so it is only necessary to remember the total number of such customers, and not the  particular customers. So, the computation needed to compute the similarity between i1 and i2 can be thought as some kind of *reduce* (by Key) operation between all pairs (i1,i2) produced by different users C. That is, if we consider (i1,i2) as the key, and for example \"1\" as the value for each different user C that bought i1 and i2, the (key,value) to produce would be:\n",
    "\n",
    "                ((i1,i2), 1)  for each user C that bought i1 and i2\n",
    "\n",
    "and then reduce by key (for example summing up the values) all such (key,value) pairs. Of course, to get a normalized similarity measure the sum should be divided by the maximun number of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format of the data set computed by such global filtering algorithm could be something like the following. For each item I we have a list of (item,similarity) pairs:\n",
    "  \n",
    " I :  [ (j1,sim(I,j1), (j2,sim(I,j2), ..., (jI,sim(I,jI) ]\n",
    " \n",
    " where the set of items j1, j2, ... , jI is the set of items that have at least one common costumer (user) with I and so their similarity is > 0. We will refer to the (distributed) data set that contains such information for all the items as the * rddSimilarityPairs * in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending similar items to a previously bought one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first recommender system, consider the case where we want to recommend similar items to one just bought by an user, or to one recently browsed by the user. So, we want to focus on similar items to a particular one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have a possible pseudo-code for a recommender system for that particular case. The input is the user we are considering and the item we want to use as the base item to get the recommendations. Observe that the primary use of this algorithm would be \"on-line\": every time an user makes a new purchase or browses a new item it would be desirable to get such focused recommendations. \n",
    "\n",
    "```python \n",
    "def Recommend_K_most_Similar( RDD rddSimilarityPairs, user U, item I, integer K ):\n",
    "\n",
    "     rdd1 = getSimilarItems( rddSimilarityPairs, I )\n",
    "     rdd2 = EraseItemsAlreadyBought( rdd1, U )\n",
    "     rdd3 = rdd2.sortBySimilarity()\n",
    "     bestK = rdd3.takeFirstKItems( K )\n",
    "     \n",
    "     return bestK\n",
    "```\n",
    "\n",
    "This function assumes that we have a previously computed data set, the rddSimilarityPairs, that should be the one computed by the similarity mining algorithm of  the previous section (or by any other algorithm that provides such data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a possible implementation in spark of the four steps of the previous algorithm. Let's first consider the following similarity information data set, that for simplicity we will consider that is stored in a plain ASCII file. In a final application this similarity information would be stored in a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1    2,0.6  4,0.3\n",
    "\n",
    "2    1,0.6\n",
    "\n",
    "3    4,0.7  5,0.8 \n",
    "\n",
    "4    3,0.7  5,0.4 1,0.3\n",
    "\n",
    "5    3,0.8  4,0.4\n",
    "\n",
    "6    7,0.3\n",
    "\n",
    "7    6,0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a parsing function to process the information of such text file to get the information for the rdd data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format of line:  ItemI    ItemI1,Sim_(I,I1) ...  ItemIN,Sim_(I,IN)\n",
    "# \n",
    "# We assume that the line contains only the information for items such that their similarity with I is > 0\n",
    "#\n",
    "def parseSimilarityInfo( line ):\n",
    "   toks = line.split()\n",
    "   sourceitem = int(toks[0])\n",
    "   targetitems = [ tuple(it.split(',')) for it in toks[1:] ]\n",
    "   targetitems = [ (int(it[0]),float(it[1])) for it in targetitems ]\n",
    "   return (sourceitem,targetitems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load and parse the information in our file similarityPairsInfo_1.txt to get the desired rdd data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddSimilarityPairs = sc.textFile('similarityPairsInfo_1.txt').map( parseSimilarityInfo )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to check if the file was well parsed (remember that perfoming a collect() action is only a good idea if the result you expect to get is small enough to fit into the driver memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [(2, 0.6), (4, 0.3)]),\n",
       " (2, [(1, 0.6)]),\n",
       " (3, [(4, 0.7), (5, 0.8)]),\n",
       " (4, [(3, 0.7), (5, 0.4), (1, 0.3)]),\n",
       " (5, [(3, 0.8), (4, 0.4)]),\n",
       " (6, [(7, 0.3)]),\n",
       " (7, [(6, 0.3)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddSimilarityPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can filter from such rdd data set only the similarity information related to our input item I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSimilarItems( rddSimilarityPairs, I ):\n",
    "   return  rddSimilarityPairs.filter( lambda x : x[0] == I )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's test the function with item 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, [(3, 0.7), (5, 0.4), (1, 0.3)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = getSimilarItems( rddSimilarityPairs, 4 )\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to retain only items not bought by user U. Again, the implementation is highly dependent on whether we assume the set of purchases of user U fits into a single machine or it must be distributed. Let's assume here that his/her ser of purchases fits into one machine, so we can read it into the following function (again we will assume such information comes from an ASCII file). The format for such user purchases will be:\n",
    "\n",
    "   user1  item11   item12 ...  item1N\n",
    "   \n",
    "   user2  item11   item12 ...  item1N\n",
    "   \n",
    "    ...\n",
    "    \n",
    "   userN  item11   item12 ...  item1N\n",
    "   \n",
    "where the set of items in line i is the set of items bought by user i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function we remove the set of already items by U, and also remove the item key to get the final set\n",
    "# similar and filtered items as a single list\n",
    "def RemoveBoughtItems( itemsSimilarToI, U ):\n",
    "   purchases = getPurchases( U )\n",
    "   print ( purchases )\n",
    "   return itemsSimilarToI.flatMap( lambda x : [it for it  in x[1] if it[0] not in purchases ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the next example file of user purchases ('purchases.txt'):\n",
    "\n",
    "    1    2  3\n",
    "    2    3  4\n",
    "    3    4  5  6\n",
    "    4    6  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPurchases( U ):\n",
    "    purchases = []\n",
    "    f = open( 'purchases.txt' )\n",
    "    for line in f:\n",
    "        toks = line.split()\n",
    "        if (int(toks[0]) == U):\n",
    "            purchases = toks[1:]\n",
    "            break\n",
    "    f.close()\n",
    "    return [ int(p) for p in purchases ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's check it filtering out the purchases of user 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 0.7), (1, 0.3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = RemoveBoughtItems( rdd1, 3 )\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two steps are sort the resulting set of items by similarity and taking the first k items. We can do this with spark with a single action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.7)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the first most similar non-bought item\n",
    "rdd2.takeOrdered(1, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending based on the whole set of previous purchases\n",
    "\n",
    "**Exercise**: Finally, consider the next modification of the previous use case as a programming exercise with spark. Sometimes Amazon also sends emails to costumers sending recommendations based on *all* (or a subset of) his/her previous purchases, instead of based on a single recent one. Observe that in this case, a same item could be recommended but with different similarity measures, as the result of being similar to different purchases of the given user (but with a different similarity value with each purchase). So, we should aggregate in some way the different similarity values obtained for a same item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Can you think about a good recommender algorithm and implement it with spark ? Take as a starting point the previous algorithm we have developed for recommendations based on a single item."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
