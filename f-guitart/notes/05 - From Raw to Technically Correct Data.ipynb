{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "99c2dc44-3917-4e28-8d1e-22c746f1023c"
    }
   },
   "source": [
    "#  From Raw to Technically Correct Data\n",
    "*(from de Jonge van der Loo)*\n",
    "\n",
    "A data set is a collection of data that describes attribute values (variables) of a number of real-world objects (units). With data that are *technically correct*, we understand a data set where each value:\n",
    "\n",
    "1. can be directly recognized as belonging to certain variable, and\n",
    "2. is stored in a data type that represents the value domain of the real-world variable.\n",
    "\n",
    "This means that for each unit, a text variable should be stored as text, a numeric variable as a number, and so on, and all this in a format that is consistent across the data set with appropriate variable (column) names. I am avoiding to comment the case in which the variable is categorical, as thus it would be necessary to use labels. I'll skip this, but you have further reading about this in the following link: http://pandas.pydata.org/pandas-docs/stable/categorical.html\n",
    "\n",
    "**The Objective is to:**\n",
    " * load data in a DataFrame with suitable columns and index names\n",
    " * have each column of the DataFrame is of the type that adequately represents the value domain of the variable in the column\n",
    " * not loose data (or at least as less as possible)\n",
    " \n",
    "**Note that:**\n",
    " * we don't check if the data makes sense\n",
    " \n",
    "\n",
    "## Reading text data to DataFrame\n",
    "\n",
    "We have already seen in previous notes that Pandas already provides libraries that get a file as an input and provide a DataFrame as an output. This is the most suitable way of reading text in Pandas, however, we always can make use of the csv library from Python, converting and converting the data to a DataFrame using any of the DataFrame constructors. \n",
    "\n",
    "We will cover some of the most common uses of Pandas regarding the csv file format reading libraries, for other file formats and further options, please refer to http://pandas.pydata.org/pandas-docs/stable/io.html.\n",
    "\n",
    "### CSV reading\n",
    "Nowadays, the most common data format is CSV, which are tabular data files which use the comma separator to divide variable data. There are two main kinds of CSV standards: comma and semi-colon separated files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "59112c70-3bbd-4bef-9666-1ec55c5f4524"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"../data/people.csv\"\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "print(content[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provide read_csv function, which takes a csv file as input, and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "734dbd76-3515-4e59-bcab-d7e40fe6c5fd"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "path_to_file = \"../data/people.csv\"\n",
    "df = pd.read_csv(path_to_file)\n",
    "display(df)\n",
    "print(\"Types:\")\n",
    "display(df.dtypes)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "efb9a000-0a42-4896-b3a9-73784cba271d"
    }
   },
   "source": [
    "**Some things to note:**\n",
    "* recognizes the first line as column names\n",
    "* recognizes variable types\n",
    "* automatically assign row index\n",
    "\n",
    "**Useful options of read_csv:**\n",
    " * **sep :** str, defaults to ',' for read_csv(), \\t for read_table()\n",
    " \n",
    "    Delimiter to use. If sep is None, will try to automatically determine this. Separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions, will force use of the python parsing engine and will ignore quotes in the data. Regex example: '\\\\r\\\\t'.\n",
    "    \n",
    "    \n",
    "* **header :** int or list of ints, default 'infer'\n",
    "\n",
    "    Row number(s) to use as the column names, and the start of the data. Default behavior is as if header=0 if no names passed, otherwise as if header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of ints that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.\n",
    "    \n",
    "    \n",
    "* **names :** array-like, default None\n",
    "\n",
    "    List of column names to use. If file contains no header row, then you should explicitly pass header=None.\n",
    "    \n",
    "    \n",
    "* **index_col :** int or sequence or False, default None\n",
    "\n",
    "    Column to use as the row labels of the DataFrame. If a sequence is given, a MultiIndex is used. If you have a malformed file with delimiters at the end of each line, you might consider index_col=False to force pandas to not use the first column as the index (row names).\n",
    "    \n",
    "    \n",
    "* **dtype :** Type name or dict of column -> type, default None\n",
    "\n",
    "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object to preserve and not interpret dtype.\n",
    "    \n",
    "    \n",
    "* **skiprows :** list-like or integer, default None\n",
    "\n",
    "    Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n",
    "    \n",
    "    \n",
    "* **skipfooter :** int, default 0\n",
    "\n",
    "    Number of lines at bottom of file to skip (unsupported with engine=’c’).\n",
    "    \n",
    "    \n",
    "* **nrows :** int, default None\n",
    "\n",
    "    Number of rows of file to read. Useful for reading pieces of large files.\n",
    "    \n",
    "* **na_values :** str, list-like or dict, default None\n",
    "\n",
    "    Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: '-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'NA', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan', ''.\n",
    "\n",
    "### XLS reading\n",
    "\n",
    "Another common file format is the xls file that contains an excel spreadsheet. In this case, each cell is a DataFrame cell, and columns are DataFrame columns and the same holds for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "country_df = pd.read_excel(\"../data/country_info_worldbank.xls\",skiprows=[0,1,2],header=1)\n",
    "country_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some things to note:**\n",
    "\n",
    "**Useful options of read_excel:**\n",
    "\n",
    "* **sheetname :** string, int, mixed list of strings/ints, or None, default 0\n",
    "\n",
    "    Strings are used for sheet names, Integers are used in zero-indexed sheet positions.\n",
    "\n",
    "    Lists of strings/integers are used to request multiple sheets.\n",
    "\n",
    "    Specify None to get all sheets.\n",
    "\n",
    "    str|int -> DataFrame is returned. list|None -> Dict of DataFrames is returned, with keys representing sheets.\n",
    "\n",
    "    Available Cases\n",
    "\n",
    "        Defaults to 0 -> 1st sheet as a DataFrame\n",
    "        1 -> 2nd sheet as a DataFrame\n",
    "        “Sheet1” -> 1st sheet as a DataFrame\n",
    "        [0,1,”Sheet5”] -> 1st, 2nd & 5th sheet as a dictionary of DataFrames\n",
    "        None -> All sheets as a dictionary of DataFrames\n",
    "\n",
    "* **header :** int, list of ints, default 0\n",
    "\n",
    "    Row (0-indexed) to use for the column labels of the parsed DataFrame. If a list of integers is passed those row positions will be combined into a MultiIndex\n",
    "\n",
    "* **skiprows :** list-like\n",
    "\n",
    "    Rows to skip at the beginning (0-indexed)\n",
    "\n",
    "* **index_col :** int, list of ints, default None\n",
    "\n",
    "    Column (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a MultiIndex\n",
    "    \n",
    "### SQL tables\n",
    "\n",
    "The use of SQL databases as data source is a very useful tool, however it won't be covered in these notes.\n",
    "\n",
    "The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. \n",
    "\n",
    "Database abstraction is provided by SQLAlchemy if installed. In addition you will need a driver library for your database.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "[http://stackoverflow.com/questions/10065051/python-pandas-and-databases-like-mysql](http://stackoverflow.com/questions/10065051/python-pandas-and-databases-like-mysql)\n",
    "\n",
    "\n",
    "### Unstructured data \n",
    "\n",
    "In previous sessions we have seen how to create DataFrames using dicts and lists. This option is still a valid option to load textfiles as a DataFrame, just by open a file as a regular text file and the parsing lines.\n",
    "\n",
    "To convert the text file to a DataFrame we will follow 5 steps:\n",
    "\n",
    "#### 1: Read data\n",
    "We can do this with the common Python approach for reading a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "fname = \"../data/unstructured_data.txt\"\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Select lines containing data\n",
    "A very useful way to do this is by using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [l for l in content if re.match(\"^(?!#).+\",l)]\n",
    "data\n",
    "header = data[0].strip(\"\\n\")\n",
    "data = data[1:]\n",
    "print([header] + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Split data into separate fields\n",
    "This can be done by applying string's split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_str = [header.split(\"/\")[1:]]+[l.split(\",\") for l in data]\n",
    "data_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Convert to DataFrame\n",
    "To standarize rows we will load the structures that we already have to a DataFrame. Remember that the constructor of a DataFrame using list needs the list to be of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_str[1:], columns=pd.Series(data_str[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5: Normalize and coerce to correct types\n",
    "We can use to_numeric. This function accepts the argument error:\n",
    "* *errors :* {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’\n",
    "\n",
    "        If ‘raise’, then invalid parsing will raise an exception\n",
    "        If ‘coerce’, then invalid parsing will be set as NaN\n",
    "        If ‘ignore’, then invalid parsing will return the input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(df[\"Age\"],errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Operations in Pandas\n",
    "\n",
    "Both DataFrames and Series have methods to apply functions over elements in the structure. These operations take a function (general, user defined or lambda) and apply it to each element value in the DataFrame or Series row or column wise.\n",
    "\n",
    "For **DataFrames** we have:\n",
    "* **apply:** applies the function to each column of the DataFrame\n",
    "* **applymap:** applies the function to each cell of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "\n",
    "f = lambda x: x.max() - x.min()\n",
    "print(df2.apply(f,axis=0))\n",
    "print(df2.apply(f,axis=1))\n",
    "\n",
    "g = lambda x: x**2\n",
    "print(df2.applymap(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Series** we have:\n",
    "* **apply:** applies the function to each element of the Series\n",
    "* **map:** applies the function to each element of the Series\n",
    "\n",
    "More about applymap, apply and map: http://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5))\n",
    "\n",
    "print(s.map(lambda x: x**2))\n",
    "print(s.apply(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation\n",
    "\n",
    "(*from http://pandas.pydata.org/pandas-docs/stable/text.html*)\n",
    "\n",
    "**Series and Index** are equipped with a set of string processing methods that make it easy to operate on each element of the array. \n",
    "\n",
    "These methods exclude missing/NA values automatically and follow (more or less) the same syntax than the bulitin string object under the str naming space. Lets use the DataFrame from unstructured dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Name\"] = df[\"Name\"].str.title()\n",
    "print(df[\"Name\"].str.len())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string methods on Index are especially useful for cleaning up or transforming DataFrame columns. For instance, you may have columns with leading or trailing whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(3, 2), columns=[' Column A ', ' Column B '],index=range(3))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since df.columns is an Index object, we can use the .str accessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.columns.str.strip())\n",
    "\n",
    "print(df.columns.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These string methods can then be used to clean up the columns as needed. Here we are removing leading and trailing whitespaces, lowercasing all names, and replacing any remaining whitespaces with underscores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some things to note:**\n",
    "1. Notice that if variables can't be coearced to any other type, Pandas will coerce to str. Its important to know how to manipulate str\n",
    "\n",
    "2. str operations can also be perfomed using apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting and Replacing Strings\n",
    "\n",
    "Methods like split return a Series of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2 = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h'])\n",
    "s2.str.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements in the split lists can be accessed using get or [] notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(s2.str.split('_').str.get(1))\n",
    "print(s2.str.split('_').str[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to expand this to return a DataFrame using expand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2.str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to limit the number of splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2.str.split('_', expand=True, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rsplit is similar to split except it works in the reverse direction, i.e., from the end of the string to the beginning of the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2.str.rsplit('_', expand=True, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like replace and findall take regular expressions, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca','', np.nan, 'CABA', 'dog', 'cat'])\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3.str.replace('^.a|dog', 'XX-XX ', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing with .str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access directly to all text positions in a text variable using str indexing with [] notation. If any strings are out of bounds, NaN is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan,'CABA', 'dog', 'cat'])\n",
    "print(s.str[0])\n",
    "print(s.str[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Substrings\n",
    "##### Extract first match in each subject (extract)\n",
    "The extract method accepts a regular expression with at least one capture group.\n",
    "\n",
    "Extracting a regular expression with more than one group returns a DataFrame with one column per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['a1', 'b2', 'c3']).str.extract('([ab])(\\d)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expand attribute to False returns a Series when possible. If we assign it to True a DataFrame will always be returned. If multiple regular expressions are grouped, the DataFrame is returned in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['a1', 'b2', 'c3']).str.extract('([ab])', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['a1', 'b2', 'c3']).str.extract('([ab])', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements that do not match return a row filled with NaN. Thus, a Series of messy strings can be “converted” into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating get() to access tuples or re.match objects. The dtype of the result is always object, even if no match is found and the result only contains NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named groups like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['a1', 'b2', 'c3']).str.extract('(?P<letter>[ab])(?P<digit>\\d)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and optional groups like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['a1', 'b2', '3']).str.extract('([ab])?(\\d)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any capture group names in the regular expression will be used for column names; otherwise capture group numbers will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more about expand attribute (e.g. when used with Index) refer to: http://pandas.pydata.org/pandas-docs/stable/text.html#extracting-substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract all matches in each subject (extractall)¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractall method returns every match. The result of extractall is always a DataFrame with a MultiIndex on its rows. The last level of the MultiIndex is named match and indicates the order in the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n",
    "two_groups = '(?P<letter>[a-z])(?P<digit>[0-9])'\n",
    "s.str.extract(two_groups, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.str.extractall(two_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to access multiindex?** \n",
    "\n",
    "You can think in multindex as a tupled index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi_df = s.str.extractall(two_groups)\n",
    "print(mi_df.columns)\n",
    "print(mi_df.index)\n",
    "print(mi_df.loc[(\"A\",0)])\n",
    "\n",
    "print(mi_df.T[\"A\",0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more: http://pandas.pydata.org/pandas-docs/stable/advanced.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use xs accessor with level parameter to filter subindex values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_result = s.str.extract(two_groups, expand=True)\n",
    "extractall_result = s.str.extractall(two_groups)\n",
    "print(extractall_result)\n",
    "print(extractall_result.xs(0, level=\"match\"))\n",
    "print(extractall_result.xs(1, level=\"match\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing for Strings that Match or Contain a Pattern\n",
    "\n",
    "You can check whether elements contain a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = r'[a-z][0-9]'\n",
    "pd.Series(['1', '2', '3a', '3b', '03c']).str.contains(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or match a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(['1', '2', '3a', '3b', '03c']).str.match(pattern, as_indexer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like match, contains, startswith, and endswith take an extra na argument so missing values can be considered True or False: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s4 = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])\n",
    "s4.str.contains('A', na=False)\n",
    "#explain that bool can be sumed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approximate String Matching\n",
    "(*from: http://stackoverflow.com/questions/13636848/is-it-possible-to-do-fuzzy-match-merge-with-python-pandas*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([[1],[2],[3],[4],[5]], \n",
    "                   index=['one','two','three','four','five'], \n",
    "                   columns=['number'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([['a'],['b'],['c'],['d'],['e']], \n",
    "                   index=['one','too','three','fours','five'], \n",
    "                   columns=['letter'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib \n",
    "\n",
    "difflib.get_close_matches\n",
    "df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])\n",
    "\n",
    "df1.join(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversion\n",
    "\n",
    "As already stated, when not possible to convert (coerce) to a defined data type, Pandas converts to str. We already worked with strings, we know how to apply functions element and column wise, so it won't be difficult to convert types by ourselves.\n",
    "\n",
    "Pandas provide some functions to convert types:\n",
    "* **to_numeric(arg[, errors]):** Convert argument to a numeric type.\n",
    "* **to_datetime(*args, **kwargs):** Convert argument to datetime.\n",
    "* **to_timedelta(*args, **kwargs):** Convert argument to timedelta\n",
    "\n",
    "###### to_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([\"10\",\"0.9\",\"56\",\"87'6\",\"34'89\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can raise, ignore or coerce\n",
    "pd.to_numeric(s,errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s[pd.to_numeric(s,errors=\"coerce\").isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use str.replace to solve this problem. Be careful at applying str over a Series. It has to contain obly str object, otherwise it will return NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(s.str.replace(\"'\",\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we recover the unstructured DataFrame, now we can convert it to the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Name\"] = df[\"Name\"].str.title().str.strip()\n",
    "df[\"Age\"] = pd.to_numeric(df[\"Age\"].str.replace(\"\\\"\",\"\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### to_timedelta:\n",
    "\n",
    "Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this does not work\n",
    "#pd.to_timedelta(df[\"Time\"].str.strip().map(lambda x: \"00:\"+x),errors=\"coerce\")\n",
    "\n",
    "#workaround\n",
    "def str_convert(x):\n",
    "    return pd.NaT if len(x)==0 else \"00:\"+x\n",
    "\n",
    "pd.to_timedelta(df[\"Time\"].str.strip().map(str_convert),box=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technically Correct Data with Pandas\n",
    "\n",
    "1. Data is stored in a DataFrame with suitable columns and row indexes\n",
    "2. Each column of the DataFrame the Pandas type that adequately represents the value domain of the variable in the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "**Exercice:** Read [iqsize.csv](https://github.com/f-guitart/data_mining/blob/master/data/iqsize.csv) using read_csv. Use na_values to assign missing data.\n",
    "\n",
    "**Exercise:** Read [country_info_worldbank.xls](https://github.com/f-guitart/data_mining/blob/master/data/country_info_worldbank.xls) using read_xls. Use skiprows to get rid of empty lines.\n",
    "\n",
    "**Exercise:** Try to load [papers.lst](papers.lst) reading it line by line taking into account that the fields are the following ones:\n",
    " * year\n",
    " * unknown\n",
    " * conference\n",
    " * authors (linked with &)\n",
    " * paper title\n",
    " \n",
    "**Exercise:** Repeat the previous exercise taking advantage of the str functions.\n",
    " \n",
    "**Exercise:** Load [books.csv](https://github.com/f-guitart/data_mining/blob/master/data/books.csv). Extract Author or Unknown. Then work with the title. Erase Irrelevant data and try to get as many authors and date as possible.\n",
    "\n",
    "**Exercise:** Convert to Technically Correct Data: [iqsize.csv](https://github.com/f-guitart/data_mining/blob/master/data/iqsize.csv). \n",
    "\n",
    "**Exercise:** Convert to Technically Correct Data: [country_info_worldbank.xls](https://github.com/f-guitart/data_mining/blob/master/data/country_info_worldbank.xls)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nbpresent": {
   "slides": {
    "221d637a-868a-4085-9fe1-7ed9020f954c": {
     "id": "221d637a-868a-4085-9fe1-7ed9020f954c",
     "prev": null,
     "regions": {
      "24be0e0e-39a4-4c4c-8c24-efd183160f4b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "99c2dc44-3917-4e28-8d1e-22c746f1023c",
        "part": "whole"
       },
       "id": "24be0e0e-39a4-4c4c-8c24-efd183160f4b"
      }
     }
    },
    "6872ad6e-cf14-4135-9eb5-70e85d06a912": {
     "id": "6872ad6e-cf14-4135-9eb5-70e85d06a912",
     "prev": "9a08f577-08c3-4415-b945-dd598a5e9dd8",
     "regions": {
      "b50e3919-dedb-428a-8991-94fc1e7ea3a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "efb9a000-0a42-4896-b3a9-73784cba271d",
        "part": "whole"
       },
       "id": "b50e3919-dedb-428a-8991-94fc1e7ea3a8"
      }
     }
    },
    "9a08f577-08c3-4415-b945-dd598a5e9dd8": {
     "id": "9a08f577-08c3-4415-b945-dd598a5e9dd8",
     "prev": "d71df4f9-4b54-464d-b845-79b5f95ba22a",
     "regions": {
      "114a3519-e738-4e44-b999-62aef3028d77": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "734dbd76-3515-4e59-bcab-d7e40fe6c5fd",
        "part": "whole"
       },
       "id": "114a3519-e738-4e44-b999-62aef3028d77"
      }
     }
    },
    "d71df4f9-4b54-464d-b845-79b5f95ba22a": {
     "id": "d71df4f9-4b54-464d-b845-79b5f95ba22a",
     "prev": "221d637a-868a-4085-9fe1-7ed9020f954c",
     "regions": {
      "5b120010-391c-4d79-993d-452cef8fdb6e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59112c70-3bbd-4bef-9666-1ec55c5f4524",
        "part": "whole"
       },
       "id": "5b120010-391c-4d79-993d-452cef8fdb6e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
